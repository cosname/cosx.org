---
title: "统计的显著性水平"
author: "侃侃迩行"
date: '2017-08-11'
output:
  blogdown::html_page:
    toc: yes
    fig_height: 4
    fig_width: 8
  pdf_document:
    includes:
      in_header: header.tex
    latex_engine: xelatex
tags: [statistics, r]
---
```{r echo = FALSE}
hh <- function (
# m0 = 80, m1 = 88, sigma = 10, n = 25, sig = 0.05,
  m0 = 0, m1 = 4, sigma = 1, n = 1, sig = 0.05,
  H0 = TRUE, H1 = TRUE, body = TRUE, tail = TRUE, beta = TRUE, power = TRUE,
  reject = TRUE, level = TRUE, AxisZ = FALSE
  )
{
# Calculate critical values
sigma_m <- sigma / sqrt(n)
d <- (m1 - m0) / sigma
# X and Ys
X0s <- seq(m0 - 5 * sigma_m, m0 + 5 * sigma_m,by = 0.01 * sigma_m)
X1s <- seq(m1 - 5 * sigma_m, m1 + 5 * sigma_m, by = 0.01 * sigma_m)
Xs <- sort(cbind(X0s, X1s))
Y0s <- dnorm(Xs, mean = m0, sd = sigma_m)
Y1s <- dnorm(Xs, mean = m1, sd = sigma_m)
# Critical X values
X_min <- qnorm(sig / 2, mean = m0, sd = sigma_m, lower.tail = TRUE)
X_max <- qnorm(sig / 2, mean = m0, sd = sigma_m, lower.tail = FALSE)
# polygons
col2alpha <- function(col, alpha) {
  col_rgb <- col2rgb(col)/255
  rgb(col_rgb[1], col_rgb[2], col_rgb[3], alpha = alpha)
}
## Tail
low_tail <- seq(min(Xs), X_min, by = sigma_m * 0.001)
high_tail <- seq (X_max, max(Xs), by = sigma_m * 0.001)
alpha_value <- sub("^0\\.", ".", formatC(sig/2, digits = 2, format = "fg"))
alpha_com <- sub("^0\\.", ".", formatC(1 - sig, digits = 2, format = "fg"))
beta_value <- pnorm(q = high_tail, mean = m1, sd = sigma_m)
beta_value <- formatC(beta_value, digits = 2, format = "fg")
beta_value <- sub("^0\\.", ".", beta_value)
power_value <- pnorm(q = high_tail, mean = m1, sd = sigma_m, lower.tail = FALSE)
power_value <- formatC(power_value, digits = 2, format = "f")
power_value <- sub("^0\\.", ".", power_value)
boddy <- seq(X_min, X_max, by = sigma_m * 0.001)
tail_x <- c(min(low_tail), low_tail, max(low_tail), min(high_tail), high_tail, max(high_tail) )
tail_y <- c(0, dnorm(low_tail, mean = m0, sd = sigma_m), 0, 0,
            dnorm(high_tail, mean = m0, sd = sigma_m), 0)
tail_col <- col2alpha("#DC143C", alpha = 0.5)
body_x <- c(min(boddy), boddy, max(boddy))
body_y <- c(0, dnorm(boddy, mean = m0, sd = sigma_m), 0)
body_col <- col2alpha("#D2691E", alpha = 0.5)
beta_range <- seq(min(min(X1s), X_max), max(min(X1s), X_max), by = sigma_m * 0.001)
beta_x <- c(min(X1s), beta_range, X_max)
beta_y <- c(0, dnorm(beta_range, mean = m1, sd = sigma_m), 0)
beta_col <- col2alpha("#00008B", alpha = 0.5)
power_range <- seq( X_max, max(X1s), by = sigma_m * 0.001)
power_x <- c(X_max, power_range, max(X1s))
power_y <- c(0, dnorm(power_range, mean = m1, sd = sigma_m), 0)
power_col <- col2alpha("#1E90FF", alpha = 0.5)
#label_pos <- seq(60, 105, by = sigma_m)
label_pos <- seq(min(Xs), max(Xs), by = sigma_m)
## plot
plot(NULL, NULL, xlim = range(Xs), # c(60, 105)
     ylim = c(0, (1 + 0.01) * max(Y0s) ),
     axes = FALSE, xlab = "", ylab = "", yaxs = "i")
# polygons
if (tail) {
  polygon(tail_x, tail_y, col = tail_col, border = "white")
  if (level){
  text(x = X_min - sigma_m * 0.5, y = dnorm(m0, m0, sd = sigma_m) / 15,
       bquote(alpha / 2 == .(alpha_value)), pos = 2)
  }
}
if (body) {
  polygon( body_x, body_y, col = body_col, border = "white")
  if (level){
  text(x = m0 - sigma_m * 0.6, y = dnorm(m0, m0, sd = sigma_m) / 2,
       bquote(1 - alpha == .(alpha_com)), pos = 3)
  }
}

if (beta) {
   polygon(beta_x, beta_y, col = beta_col, border = "white")
   if (level){
   text(x = X_max - sigma_m * 0.4, y = dnorm(m1, m1, sd = sigma_m) / 15,
        labels = bquote(beta == .(beta_value)), pos = 2)
   }
}
if (power) {
   polygon(power_x, power_y, col = power_col, border = "white")
   if (level){
   text(x = X_max + abs(m1 - X_max) * 0.5,
        y = dnorm(m1, m1, sd = sigma_m) / 15,
        labels = bquote(1-beta == .(power_value)), pos = 4)
   }
}
# H_0
if (H0){
lines(Xs, Y0s, col = "#cf232a", lwd = 2)
abline(v = m0, lwd = 1, col = "gray")
mtext(expression(H[0]), side = 3, at = m0)
arrows(x0 = m0, y0 = dnorm(m0 + sigma_m, m0, sigma_m),
       x1 = m0 + sigma_m, y1 = dnorm(m0 + sigma_m, m0, sigma_m),
       length = 0.1,  col = "gray")
text(labels = bquote(sigma[M] == .(round(sigma_m, 2))), x = m0 + sigma_m / 2,
     y = dnorm(m0 + sigma_m, m0, sigma_m), pos = 3, col = "gray")
}
# H_1
if (H1){
lines(Xs, Y1s, col = "#d5493a", lwd = 2)
abline(v = m1, lwd = 1, col = "gray")
mtext(expression(H[1]),  side = 3, at = m1)
mtext(text = bquote("Cohen's d" == .(d)~", n"==.(n)), side = 3, at = (m0 + m1) / 2,
      padj = -2)
}
## X-Axes

axis(1, at = label_pos, label = round(label_pos, 2))
mtext(text = "X", side = 1, adj = 1, line = -0.6)
if (AxisZ){
axis(side = 1, at = label_pos, label = (label_pos - m0) / sigma_m , line = 2.5)
mtext(text = "Z", side = 1, adj = 1, line = 1.9)
}

## Critical lines
if(reject){
abline(v = X_max, lty = 3)
text( x = X_max, y = dnorm(m0, m0, sd = sigma_m) / 1.1,
    label = expression(paste("Reject ", H[0])),  pos = 4)
arrows(x0 = X_max, y0 = dnorm(m0, m0, sd = sigma_m) / 1.16,
    x1 = X_max + sigma_m, length = 0.1)
abline(v = X_min, lty = 3)
text(x = X_min, y = dnorm(m0, m0, sd = sigma_m) / 1.1,
    label = expression(paste("Reject ", H[0])), pos = 2)
arrows(x0 = X_min, y0 = dnorm(m0, m0, sd = sigma_m) / 1.16,
       x1 = X_min - sigma_m, length = 0.1)
}
}

############################
pBF <- function(
type1 = 0.05, # alpha level is 0.05/2
type2 = 0.25, # beta is 0.25, so that power is 1-0.25 = 0.75
UMPTB = 0.005
){
p = 1 - c(9000 : 9990) / 10000
xbar = qnorm(p = 1 - p / 2, mean = 0, sd = 1)

## alternative based on 80% POWER IN 5% TEST
muPower = qnorm(p = 1 - type2, mean = 0, sd = 1) +
          qnorm(p = 1 - type1 / 2, mean = 0, sd = 1)  # mu of the alternative with the power of 75%
bfPow = 0.5 * (dnorm(x = xbar, mean =  muPower, sd = 1) +
               dnorm(x = xbar, mean = -muPower, sd = 1)) /
               dnorm(x = xbar, mean = 0, sd = 1)
muUMPBT = qnorm(p = 1 - UMPTB / 2, mean = 0, sd = 1)
bfUMPBT = 0.5 * (dnorm(x = xbar, mean =  muUMPBT, sd = 1) +
                 dnorm(x = xbar, mean = -muUMPBT, sd = 1)) /
                 dnorm(x = xbar, mean = 0, sd = 1)

## two-sided "LR" bound
bfLR = 0.5 / exp(- 0.5 * xbar ^ 2)
bfLocal = -1 / (exp(1) * p * log(p))  # Local bound; no need for two-sided adjustment
yy <- cbind(bfLR, bfLocal)

## Coordinates for dashed lines
data = data.frame(p, bfLocal, bfLR, bfPow, bfUMPBT)
U_005 = max(data[data$p == "0.005", -1])
L_005 = min(data[data$p == "0.005", -1])
U_05  = max(data[data$p == "0.05", -1])
L_05  = min(data[data$p == "0.05", -1])

# plot margins
# par(family = "Source Han Serif CN") # STKaiti
par(mai = c(0.8, 0.8, 0.1, 0.6))
par(mgp = c(2, 1, 0))
matplot(p, yy, type = 'n', log = 'xy',
        xlab = expression(paste(italic(P) , "-value")),
        ylab = "Bayes Factor",
        ylim = c(0.3, 100), bty = "n", xaxt = "n", yaxt = "n")
lines(p, bfPow, col = "red", lty = 1, lwd = 2)
lines(p, bfLR, col = "black", lty = 2, lwd = 2)
lines(p, bfUMPBT, col = "blue", lty = 3, lwd = 2)
lines(p, bfLocal, col = "#56B4E9", lty = 4, lwd = 2)
legend(x = 0.015, y = 100,
       legend = c(expression(paste("Power")), "Likelihood Ratio Bound", "UMPBT",
                  expression(paste("Local-", italic(H)[1]," Bound"))),
       lty = 1:4, lwd = 2, col = c("red", "black", "blue", "#56B4E9"), cex = 0.8)
## customizing axes
# x axis
axis(side = 1,
  at = c(-2, 0.001, 0.0025, 0.005, 0.010, 0.025, 0.050, 0.100, 0.14),
  labels = c("", "0.0010", "0.0025", "0.0050", "0.0100", "0.0250", "0.0500", "0.1000", ""),
  lwd = 1, tck = -0.01, padj = -1.1, cex.axis = 0.8)

# y axis on the left - main
axis(side = 2,
     at = c(-0.2, 0.3, 0.5, 1, 2, 5, 10, 20, 50, 100),
     labels = c("", "0.3", "0.5", "1.0", "2.0", "5.0", "10.0", "20.0", "50.0", "100.0"),
     lwd = 1, las = 1, tck = -0.01, hadj = 0.6, cex.axis = 0.8)

#y axis on the left - secondary (red labels)
axis(side = 2, at = c(L_005, U_005),
     labels = c(formatC(L_005, digits = 2, format = "f"), formatC(U_005, digits = 2, format = "f")),
     lwd = 1, las = 1, tck = -0.01,
     hadj = 0.6, cex.axis = 0.6, col.axis = "red")

#y axis on the right - main
axis(side = 4, at = c(-0.2, 0.3, 0.5, 1, 2, 5, 10, 20, 50, 100),
     labels = c("","0.3","0.5","1.0","2.0","5.0","10.0","20.0","50.0","100.0"),
     lwd = 1, las= 1, tck = -0.01, hadj = 0.4, cex.axis = 0.8)

# y axis on the right - secondary (red labels)
axis(side = 4, at = c(L_05, U_05),
     labels = c(formatC(L_05, digits = 2, format = "f"), formatC(U_05, digits = 2, format = "f")),
     lwd = 1, las = 1, tck = -0.01, hadj = 0.4, cex.axis = 0.6, col.axis = "red")

### dashed lines
segments(x0 = 0.000011, y0 = U_005, x1 = 0.005, y1 = U_005, col = "gray40", lty = 2)
segments(x0 = 0.000011, y0 = L_005, x1 = 0.005, y1 = L_005, col = "gray40", lty = 2)
segments(x0 = 0.005, y0 = 0.00000001, x1 = 0.005, y1 = U_005, col = "gray40", lty = 2)
segments(x0 = 0.05, y0 = U_05, x1 = 0.14, y1 = U_05, col = "gray40", lty = 2)
segments(x0 = 0.05, y0 = L_05, x1 = 0.14, y1 = L_05, col = "gray40", lty = 2)
segments(x0 = 0.05, y0 = 0.00000001, x1 = 0.05, y1 = U_05, col = "gray40", lty = 2)
}


############

ppp <- function(){
#graph margins
pow1 = c(5 : 999) / 1000 # power range for 0.005 tests
pow2 = c(50 : 999) / 1000 # power range for 0.05 tests

pow   <- c("pow1", "pow2", "pow2", "pow1",  "pow2",  "pow1")
alpha <- c( 0.005,   0.05,   0.05,  0.005,    0.05,   0.005)
pi0   <- c(   5/6,    5/6,  10/11,  10/11,   40/41,   40/41)
lty   <- c(     1,      2,      2,      1,       2,       1)
col   <- c("blue", "blue",  "red",  "red", "green", "green")
df <- data.frame(pow, alpha, pi0, lty, col, stringsAsFactors=FALSE)

ddata <- function(
 pow = "pow1",
 alpha = 0.005,
 pi0 = 5/6,
 id
){
 N = 10^6
 pow = eval(parse(text = pow))
 # yy = alpha * N * pi0 / (alpha * N * pi0 + pow[id] * (1 - pi0) * N)
 yy = alpha * pi0 /(alpha * pi0 + pow[id] * (1 - pi0))
 return(yy)
}

lline <- function(x
){
pow = df[x, 1]
alpha = df[x, 2]
pi0 = df[x, 3]
lty = df[x, 4]
col = df[x, 5]
xx = eval(parse(text = pow))
yy = ddata(pow = pow, alpha = alpha, pi0 = pi0)
lines(xx, yy, lty = lty, col = col, lwd = 2)
}

# png("Figure_2.png", width = 10, height = 8, units = "in", res = 600) # Print the plot
#par(family = "Source Han Serif CN") # STKaiti
par(mai = c(0.8, 0.8, 0.1, 0.1))
par(mgp = c(2, 1, 0))
plot(x = pow1, y = ddata(), type='n', ylim = c(0, 1), xlim = c(0, 1.5),
     xlab = 'Power',
     ylab = 'False positive rate',
     bty = "n", xaxt = "n", yaxt = "n")
#grid lines
segments(x0 = -0.058, y0 = (0:5) * 0.2, x1 = 1, lty = 1, col = "gray92")
for (x in 1:6) lline(x)

#customizing axes
aat <- c(-0.5, 0, 0.2, 0.4, 0.6, 0.8, 1.0)
llabels <- c("", "0.0", "0.2", "0.4", "0.6", "0.8", "1.0")
llegend <- c("Prior odds = 1:40", "Prior odds = 1:10", "Prior odds = 1:5")
#llegend <- c("先验概率 = 1:40", "先验概率 = 1:10", "先验概率 = 1:5")
axis(side = 2, at = aat, labels = llabels,
  lwd = 1, las = 1, tck = -0.01, hadj = 0.4, cex.axis = .8)
axis(side = 1, at = aat, labels = llabels,
  lwd = 1, las = 1, tck = -0.01, padj = -1.1, cex.axis = .8)
legend(x = 1.05, y = 1, legend = llegend,
  pch = c(15, 15, 15), col = c("green", "red", "blue"), cex = 1)

library(pBrackets)
odd_1_5_1  <- ddata(pow = "pow1", alpha = 0.005, pi0 = 5/6, id = 995)
odd_1_5_2  <- ddata(pow = "pow2", alpha = 0.05, pi0 = 5/6, id = 950)
odd_1_10_2 <- ddata(pow = "pow2", alpha = 0.05, pi0 = 10/11, id = 950)
odd_1_10_1 <- ddata(pow = "pow1", alpha = 0.005, pi0 = 10/11, id = 995)
odd_1_40_2 <- ddata(pow = "pow2", alpha = 0.05, pi0 = 40/41, id = 950)
odd_1_40_1 <- ddata(pow = "pow1", alpha = 0.005, pi0 = 40/41, id = 995)

t1label <- expression(paste(italic(P), " < 0.05 threshold"))
t2label <- expression(paste(italic(P), " < 0.005 threshold"))
text(1.11, (odd_1_5_2 + odd_1_40_2)/2, label = t1label, cex = 0.9,adj=0)
text(1.11, (odd_1_5_1 + odd_1_40_1)/2, label = t2label, cex = 0.9,adj=0)
brackets(1.03, odd_1_40_1, 1.03, odd_1_5_1, h = NULL, ticks = 0.5, curvature = 0.7, type = 1, col = 1, lwd = 1, lty = 1, xpd = FALSE)
brackets(1.03, odd_1_40_2, 1.03, odd_1_5_2, h = NULL, ticks = 0.5, curvature = 0.7, type = 1, col = 1, lwd = 1, lty = 1, xpd = FALSE)
#dev.off()
}
```

提高某些领域研究可重复性的一个直接和简单的方法是提高假设检验的统计显著性水平。

# 1. 统计检验和显著性水平 {#hypothesis-testing}

统计检验中，研究者最终希望确定零假设 ($H_{i,i=0}$) 和备则假设 ($H_{i, i \neq 0}$)  中哪一个正确。由于这两种状态的不相容性，事实情况只有两种：要么是零假设正确；要么是其中一个备择假设如 $H_1$ 正确。让事情变得更为复杂的是，研究者用于判断哪个假设正确的证据并不来自于数据整体，而来自于整体数据的一部分，即一个样本。样本统计量对于总体参数来说始终存在一定偏差，即抽样误差。

第一、假如事实情况是零假设正确。假定零假设 ($H_0$) 整体服从平均值为 $\mu_0 = 0$, 标准差为 $\sigma = 1$ 的正态分布。即使在该零假设正确的前提下，由于抽样误差的原因，一个样本量为 $n=1$ 的随机样本的样本平均值 ($\overline{X}_i$) 理论上来说可以是位于 $[-\infty, +\infty]$ 之间的任何一个数。但是从概率上来讲，样本平均值在总体平均值 0 的位置出现的概率最高；一个位置离总体平均值越远，样本平均值在该位置出现的概率就越低。例如，一个样本平均值出现在 $\pm 3$ 位置上的概率就非常低了。
以样本平均值的大小为横坐标，该样本平均值出现的概率为纵坐标，可以画出如下样本平均值的概率密度分布图。

```{r label = "Figure_1_01", echo = FALSE, fig.align = "center"}
hh(H0 = T, H1 = F, body = F, tail = F, beta = F, power = F, level =  F, reject = F)
```

*P*-值指在零假设正确的前提下，样本平均值 ($\overline{X}_i$) 与某个观测值 ($\overline{X}_\text{obs}$) 一样极端或者比观测值更极端的概率。如观测值 ($\overline{X}_\text{obs} = -1$) 对应的 *P* 值为 $P=0.16$。

```{r label = "Figure_1_02", echo = FALSE, fig.align = "center"}
hh(sig = 2 * pnorm(-1), H0 = T, H1 = F, body = F, tail = T, beta = F, power = F, level = T, reject = F)
```

如前所述，一个位置离总体平均值越远，样本平均值在该位置出现的概率就越低。当一个实际观测值 ($\overline{X}_\text{obs}$) 离总体平均值足够远的时候，我们就有理由怀疑该样本不是来自零假设条件下的总体，而是来自于另外一个总体了。这个人为设定的**足够远**，即为 $\alpha$ 水平。心理学学家通常把该临界值定为 $\alpha=0.05$ (如下图)。如果观测值 ($\overline{X}_\text{obs}$) 大于等于 1.96, 即 $|\overline{X}_\text{obs}|\geq 1.96$，或其在分布中出现的概率小于等于 0.05, 研究者则断定该样本并不来自于零假设整体，而是来自某个备择假设整体。如图所示，$\alpha$ 水平又等价于当零假设实际上正确，但研究者拒绝零假设的概率，即 **I 类错误水平**。

```{r label = "Figure_1_03", echo = FALSE, fig.align = "center"}
hh(H0 = T, H1 = F, body = F, tail = T, beta = F, power = F, level = T, reject = T)
```
第二、假设事实情况是零假设 ($H_0$) 错误，备择假设 ($H_1$) 正确。同时假定零假设 ($H_0$) 和备择假设 ($H_1$) 间效应值大小为 $\text{Cohen's D} = 4$，即备择假设 ($H_1$) 整体服从平均值为 $\mu_0 = 4$, 标准差为 $\sigma = 1$ 的正态分布。如果依然把显著性水平定为 $\alpha=0.05$，即当 $P \leq 0.05$ 或观测值 $\overline{X}_{obs} \geq 1.96$ 时，拒绝零假设。 因为实际情况是备择假设 ($H_1$) 正确 (如下图)，此时样本平均值 ($\overline{X}_i$) 有 $1-\beta=0.98$ 的可能性位于临界值 1.96 的右侧，另有 $\beta = 0.021$ 的可能性位于临界值的左侧。所以当 $\overline{X}_{obs} \geq 1.96$ 时，如果我们拒绝零假设($H_0$)，接受备则假设 $H_1$，那么我们的判断正确的概率是 $1-\beta = 0.98$。正确拒绝零假设的概率，$1-\beta$ 又被称作统计效力 (statistical power)。因为在备择假设 ($H_1$) 正确的前提下，样本平均值 ($\overline{X}_i$) 仍有  $\beta = 0.021$ 的可能性位于临界值的左侧。所以当观测值位于临界值左侧 $\overline{X}_{obs} < 1.96$，我们的结论是无法拒绝零假设时，我们有 $\beta = 0.021$ 的可能性是错的。此时研究者犯的是 **II 类错误**，即实际上零假设 $H_0$ 错误，备择假设 $H_1$ 正确，但根据观测数据研究者无法拒绝零假设的情况。如图所示，II 类错误的概率是 $\beta$。

```{r label = "Figure_1_04", echo = FALSE, fig.align = "center"}
hh(H0 = T, H1 = T, body = F, tail = F, beta = T, power = T, level = T, reject = T)
```

上述四种情况可以总结为下面这个表格：


|              |$H_0$        |$H_1$        |
|--------------|:--------:|:--------:|
|没有拒绝零假设|正确拒斥（$1-\alpha$）      |漏报（II类错误，$\beta$值）|
|拒绝零假设    |虚报（I类错误，$\alpha$值）|击中（统计效力，$1-\beta$）|

以及下面这个图：

```{r label = "Figure_1_05", echo = FALSE, fig.align = "center"}
hh()
```

# 2. 显著性水平和贝叶斯因子 {#bf}

如前所述，*P* 值指在零假设 ($H_0$) 正确的条件下，研究者记录到某个观测值 ($\overline{X}_{obs}$) 的概率，即 $Pr(\overline{X}_{obs}|H_0)$。而研究者关心的是记录到该观测值时零假设或备择假设正确与否的概率，即$Pr(H_0|\overline{X}_{obs})$ 或 $Pr(H_1|\overline{X}_{obs})$ 概率的大小。大多数情况下，研究者通常无法以一种二元论的方式去确定零假设还是备择假设正确，而只能从概率上哪讲哪个假设正确的可能性更大。判断哪个可能性更大的更直接证据来自于备择假设 $H_1$ 和零假设 $H_0$ 发生概率的比率 (Odds)。如下图所示，根据贝叶斯原则，在观测值 $\overline{X}_{obs}$  条件下，备择假设和零假设发生率之比受**贝叶斯因子** (Bayes factor, **BF**) 和**先验概率** (prior odds) 两个因素的共同影响。

$$
\frac{\text{Pr }(H_1|\overline{X}_{obs})}{\text{Pr }(H_0|\overline{X}_{obs})}=
\frac{\text{Pr }(\overline{X}_{obs}|H_1)}{\text{Pr }(\overline{X}_{obs}|H_0)}\times
\frac{\text{Pr }(H_1)}{\text{Pr }(H_0)}=
BF \times (\text{prior odds})
$$

第一，关于备择假设和零假设发生概率之比的**先验知识** (prior odds) 通常来源于研究者的前期假设、相关领域科学研究的共识、或同领域相似研究问题的其他研究。通常认为，心理学研究和癌症临床研究中，备择假设和零假设的先验比率约为 **1:10**。而在前临床的生物医学研究 (preclinical biomedical reaearch) 领域，该先验比率会更低。

第二、 **贝叶斯因子**来源于观测数据，贝叶斯因子的大小表明了证据的强度。贝叶斯因子越大，观测数据越有能力支持备择假设假设。按照 Kass 和 Raftery (1995) 的观点，贝叶斯因子的大小和证据强度 (strength of evidence) 存在以下关系：

|贝叶斯因子大小|否定零假设的证据强度|
|:---:|:---:|
|1-3.2|不值一提|
|3.2-10|显著的 (Substantial)|
|10-100|强 (Strong)|
|>100|确定的 (Decisive)|

贝叶斯因子和 *P*-值之间的关系受备择假设的影响。要确定贝叶斯因子和 *P*-值之间的关系，首先要确定备择假设 $\overline{H}_{i,i\neq0}$ 的分布特征。确定备择假设的方法不同，计算出的贝叶斯因子和 *P*-值的关系也不同。如果我们假定备择假设和零假设条件下整体分布的形状一致，那么确定备择假设的分布特征就是确定其整体平均值 $\mu_1$。

- 第一种方法利用显著性检验中的**统计效力**大小确定备择假设的整体平均值。例如，备择假设整体平均值 $\mu_1$ 的大小必须让被则假设满足其统计效力为 75%。根据统计效力大小确定备择假设后，观察到的 *P*-值大小和贝叶斯因子大小之间的关系叫做 **效力曲线** (power)。

```{r label = "Figure_1_06", echo = FALSE, fig.align = "center"}
mm1 <- qnorm(1 - 0.05/2) + qnorm(0.75)
hh(m1 = mm1)
```

- 第二种方法是通过显著性水平来确定备择假设的整体平均值的。Johnson (2013) 认为备择假设的整体平均值应该设定在 $\alpha／2 = 0.0025$ 处。这种方法叫做**均匀最大功效贝叶斯检验** (Uniformly Most Powerful Bayesian Test, UMPBT)。在我们的例子中，零假设为真时 $\alpha／2 = 0.0025$ 对应的 $\overline{X}_{obs}=$ `r formatC(qnorm(1 - 0.005/2), digits = 2, format = "f")`，所以备择假设的整体平均值为  $\mu_1=$ `r formatC(qnorm(1 - 0.005/2), digits = 2, format = "f")`。因为当备择假设的整体平均值为 $\mu_1=$ `r formatC(qnorm(1-0.005/2), digits = 2, format = "f")` 时，显著性检验的统计效力为 $1-\beta=0.80$，所以该方法确定的备择假设与第一种方法中把统计效力设定为 $80\%$ 时的出来的 $P$-值和贝叶斯因子之间的关系是一样的。

```{r label = "Figure_1_07", echo = FALSE, fig.align = "center"}
mm1 <- qnorm(1 - 0.005/2)
hh(m1 = mm1)
```

后两种方法没有试图找到一个特定备择假设，而是在假定备择假设具有某种特点时，去描述 *P*-值和贝叶斯因子的关系。

- 第三种方法认为如果备择假设 $H_{i}$ 的整体平均值 $\mu_i$ 以零假设 $H_{i, i=0}$ 的整体平均值 $\mu_0$ 为中心的对称分布时，那么贝叶斯因子的上限将不超过 $BF = 1/(2\times \exp(\frac{1}{2}t^2))$，此处 $t$ 指 I 类错误的大小。此种方法被称作 **似然值比率范围** (likelihood ratio bound) (Berger, & Sellke, 1987)
- 第四种方法认为当备择假设 $H_{i}$ 的整体平均值 $\mu_i$ 是一个以零假设 $H_{i, i=0}$ 的整体平均值 $\mu_0$ 为众数的单峰分布 (unimodal)，并满足某些限制条件时，贝叶斯因子的上限将不超过 $BF=1/(-\exp(1) \cdot p \ \cdot \ln(p))$，$p$ 即为 $p$-值的大小。此方法被称作**局部 $H_1$ 范围** (Local-$H_1$ Bound) (Sellke, Bayarri, & Berger, 2001)

用上述四种方法确定备择假设或者假定备择假设具有某种特征后，*P*-值和贝叶斯因子的关系可以用下图表示(Benjamin et al., 2017)：

```{r label = "Figure_2_01", echo = FALSE, fig.align = "center"}
pBF()
```
如上图所示，显著性水平为 $P < .05$ 的双尾检验所对应的贝叶斯因子仅为 $2.5 - 3.4$。根据前面的描述，此时否定零假设的证据是非常弱的。如果我们把研究的先验概率设定为 $1:10$，此时备择假设发生的可能性仅为零假设发生可能性的三分之一，即 $\frac{1}{3} = 3.4 \times \frac{1}{10}$。而当我们把显著性水平设定为 $p < .005$ 的水平上时，贝叶斯因子就增加到了 $13.80-25.70$。根据前述标准，此时证据力度是很强的。在同样的先验概率下，备择假设和零假设的比率增加到了 $2.57$。

# 3. 显著性水平和假阳性率 {#fpr}

在贝叶斯框架下，*P*-值的一个对应替代方案是**假阳性率**（false positive rate，FPR)。错误发现率指在所有零假设被拒绝的情景中，零假设实际上为真的概率。假设一项研究中，零假设 ($H_0$) 为真的概率为 $\phi$，备择假设 ($H_i$) 为真概率则为 $1-\phi$。如果把假设检验的临界值设定为 $\alpha$， 即当 $P < \alpha$ 时拒绝零假设 ($H_0$)。因为零假设发生的概率是 $\phi$，那么研究者错误拒绝零假设的概率为 $\alpha\phi$。另外如果该假设检验的统计效力为 $1-\beta$，那么研究者正确拒绝零假设的概率为 $(1-\beta)(1-\phi)$。
此时错误发现率可以表示为错误决绝零假设的概率与所有拒绝零假设的概率的比率，如下图：

$$
\text{false positive rate} \approx
\frac
{\alpha\phi}
{\alpha\phi + (1-\beta)(1-\alpha)}
$$

如上面的公式所示，假阳性率同时受显著性水平、先验概率和统计效力的共同影响。在选取了三个先验概率水平 $1:40$、$1:10$ 和 $1:5$，两个显著性水平 $P <.05$ 和 $p <.005$ 后，我们可以看到假阳性率和统计效力存在下图所示的关系：

```{r label = "Figure_3_01", echo = FALSE, fig.align = "center"}
ppp()
```

如上图所示，低统计效力加上低显著性水平 $\alpha = 0.05$ 将产生高比率的假阳性率。如果显著性水平设置为 $P < 0.05$，先验概率设定为 *1:10*，那么无论统计效力处于什么水平，该分析的假阳性率将**不低于 33%**。心理学及许多科学研究的一个事实是统计效力都比较低。而如果把显著性水提高到 *0.005*，则在绝大部分统计效力范围内，假阳性率可降到 $5\%$ 之下。

# 4. 显著性水平和统计效力 {#power}

提高统计显著性水平的缺点也是显而易见的，即提高统计的显著性水平将显著降低假设检验的统计效力、提高 II 类错误的概率。如下图所示，在 $\alpha=0.05$ 的显著性水平下，该显著性检验的统计效力为 $1-\beta=0.80$ 。
```{r label = "Figure_1_08", echo = FALSE, fig.align = "center"}
hh(m1 = 1.25, sigma = 2, n = 20)
```

但当我们把统计显著性提高到 $\alpha=0.005$ 时，该检验的统计效力就降低到了 $1-\beta=0.50$。

```{r label = "Figure_1_09", echo = FALSE, fig.align = "center"}
hh(m1 = 1.25, sig = 0.005, sigma = 2, n = 20)
```

此时，如果想保持原来的统计效力水平，研究者就需要提高研究的样本量（实际应用中，这将增加项目的研究成本）。如下图所示，如果我们把样本量增加 70%，检验的统计效力将恢复到 $1-\beta=0.80$ 的水平。

```{r label = "Figure_1_10", echo = FALSE, fig.align = "center"}
hh(m1 = 1.25, sig = 0.005, sigma = 2, n = 20 * 1.7 )
```

# 5. 总结 {#summary}

如前所述，提高假设检验中的统计显著性水平能显著提高统计检验的贝叶斯因子水平、显著降低统计检验的假阳性率。在实践中，提高统计显著性水平也能显著提高科学研究的可重复性 (Benjamin et al., 2017)。正是基于上述原因，目前有 72 为统计学家联名发文希望在传统上使用两个标准差 ($\alpha = .05$) 作为显著性水平的研究领域应该把显著性水平提高到三个标准差 ($\alpha = .005$) (Benjamin et al., 2017)。

在某些先验概率非常低的探索性研究领域，研究者需要远比 0.005 更为严苛的显著性水平。如通常认为基因和高能物理研究领域的显著性水平应该符合五个标准差原则 (5-sigma)，即显著性水平大约为 $P < 3\times 10^{-7}$。

提高统计的显著性水平并不是解决研究可重复性问题的唯一方法。其他可能的解决方案包括选择更为合适的统计方法，如模型比较和贝叶斯学习等、更周到的实验设计、预实验等等。当然，改变 *P*-值的显著性水平最为直接和最为简单的方法。

# 6. 参考文献 {#references}

Altman, N., & Krzywinski, M. (2017). Points of Significance: Interpreting P values. *Nature Methods, 14*(3), 213-214. doi:10.1038/nmeth.4210

Benjamin, D. J., et al.,. (2017). Redefine Statistical Significance. *Nature Human Behavior*. doi:10.1038/s41562-017-0189-z

Berger, J. O., & Sellke, T. (1987). Testing a Point Null Hypothesis: The Irreconcilability of P Values and Evidence. *Journal of the American Statistical Association, 82*(397), 112-122.

Chawla, D. S. (2017). P-value shake-up proposed. *Nature, 548*, 16-17. doi:10.1038/nature.2017.22375

Johnson, V. E. (2013). Revised standards for statistical evidence. *Proceedings of the National Academy of Sciences of the United States of America, 110*(48), 19313-19317. doi:10.1073/pnas.1313476110

Kass, R. E., & Raftery, A. E. (1995). Bayes Factors. *Journal of the American Statistical Association, 90*(430), 773-795. doi:10.1080/01621459.1995.10476572

Nuzzo, B. (2014). Statistical errors. *Nature, 506*, 150-152. doi:http://doi.org/10.1038/506150a

Sellke, T., Bayarri, M. J., & Berger, J. O. (2001). Calibration of  p Values for Testing Precise Null Hypotheses. *The American Statistician, 55*(1), 62-71. doi:10.1198/000313001300339950

Wasserstein, R. L., & Lazar, N. A. (2016). The ASA‘s Statement on p-Values: Context, Process, and Purpose. *The American Statistician, 70*(2), 129-133. doi:10.1080/00031305.2016.1154108
