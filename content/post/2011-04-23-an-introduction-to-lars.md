---
title: LARS算法简介
date: '2011-04-23T14:53:19+00:00'
author: 郝智恒
categories:
  - 回归分析
  - 统计计算
tags:
  - Efron
  - lars
  - lasso
slug: an-introduction-to-lars
forum_id: 418836
---

最近临时抱佛脚，为了讨论班报告Group Regression方面的文章，研究了Efron等人于2004年发表在Annals of Statistics里一篇被讨论的文章LEAST ANGLE REGRESSION。这篇文章很长，有45页。加上后面一些模型方面大牛的讨论的文章，一共有93页。对于这种超长论文，我向来敬畏。后来因为要报告的文章里很多东西都看不懂，才回过头来研读这篇基石性的文章。

所谓大牛，就是他能提出一种别人从来没有提出过的想法。大牛们看待问题的角度和常人不同。比如在回归中常用的逐步回归法。我们小辈们只知道向前回归，向后回归还有二者结合的一些最基本的想法。比如向前回归，就是先选择和响应最相关的变量，进行最小二乘回归。然后在这个模型的基础上，再选择和此时残差相关度最高的（也就是相关度次高）的变量，加入模型重新最小二乘回归。之后再如法继续，直到在某些度量模型的最优性准则之下达到最优，从而选取一个最优的变量子集进行回归分析，得到的模型是相比原模型更加简便，更易于解释的。这种方法，牺牲了模型准确性（预测有偏），但是提高了模型的精确度（方差变小）。大多数本科生对逐步回归的理解也就如此了。Efron看待这个问题时，比起常人更高了一个层次。他首先指出，逐步向前回归，有可能在第二步挑选变量的时候去掉和X1相关的，但是也很重要的解释变量。这是因为它每次找到变量，前进的步伐都太大了，侵略性太强。

<!--more-->

因此在这个基础上，Efron提出了Forward stagewise。也就是先找出和响应最相关的一个变量，找到第一个变量后不急于做最小二乘回归，而是在变量的solution path上一点一点的前进(所谓solution path是指一个方向，逐步回归是在这个方向上进行)，每前进一点，都要计算一下当前的残差和原有的所有变量的相关系数，找出绝对值最大的相关系数对应的变量。我们可以想像，刚开始，前进的步伐很小，相关系数绝对值最大的对应的变量一定还是第一步选入的变量。但是随着前进的进程不断向前，这个相关系数的绝对值是在慢慢减小的，直到找到另外一个变量X2，它和当前前残差的相关系数和第一个入选变量X1的相关系数绝对值相同，并列第一。此时把X2也加入回归模型中，此时回归模型在X1上的系数已经确定了，如果在X1的solution path上继续前进，则得到的与当前残差相关系数最大的变量一定是X2，所以不再前进，而是改为在X2的solution path上前进，直到找到第三个变量X3，使得X3的与当前残差的相关系数绝对值最大。这样一步一步进行下去。每一步都是很多小步组成。直到某个模型判定准则生效，停止这个步骤。在每一个solution path上的计算都是线性的。总体的solution path是分段线性的。这种算法是一种自动进行模型构建的方法。它和传统的Forward selection在本质上是一样的，都是选择一个变量，然后选择一个继续进行的solution path，在该方向上前进。这两种方法的solution path的选择方法是一样的，唯一的区别就是前进的步伐不一样，Forward selection的前进步伐很大，一次到头，而stagewise则是一小步一小步前进。这样比Forward selection要谨慎一些，会免于漏掉一些重要的变量。

从这个视角来看，我们可以选择另外一种solution path。Efron等人在这篇文章中，就提出了一种新的solution path。在已经入选的变量中，寻找一个新的路径，使得在这个路径上前进时，当前残差与已入选变量的相关系数都是相同的。直到找出新的与当前残差相关系数最大的变量。从几何上来看，当前残差在那些已选入回归集的变量们所构成的空间中的投影，是这些变量的角平分线。下面我简单的描述一下这个算法：

* 第一步，我们初始的估计模型为0，那么当前的残差就是Y,我们找出X’Y中绝对值最大的那个对应的变量，记为X1,把它加入回归模型。这一步中X’Y是当前残差和所有变量的相关系数向量。（注意这里Y都已经中心化，X中心标准化过了）。

* 第二步，在已选的变量的solution path上前进，solution path就是s1*X1，s1是X1与当前残差的相关系数的符号。在这个path上前进，直到另外一个变量出现，使得X1与当前残差的相关系数与它和当前残差的相关系数相同。记这个变量为X2，把它加入回归模型中。

* 第三步，找到新的solution path。Efron在文章中提出了一种找出满足LARS条件的solution path的解法。solution path需要使得已选入模型变量和当前残差的相关系数均相等。因此这样的路径选择它的方向很显然就是`$X_k(X_k’X_k)^{-1}1$`的指向（因为`$X_k'(X_k(X_k’X_k)^{-1})1$`的元素都相同，保证了LARS的要求，当然这里或许会有一些其他的解，也能满足LARS的要求，有没有达人能想到或许证明这个解是唯一的）。只要再标准化这个向量，我们便就找到了solution path的方向。在这个方向上前进，直到下一个满足与当前残差相关系数绝对值最大的变量出现。如此继续下去。

LARS算法，保证了所有入选回归模型的变量在solution path上前进的时候，与当前残差的相关系数都是一样的。这一点，比起Forward stagewise要捷径一些，走得更快一些。

LARS算法已经在SAS和R中实现了。作为回归模型选择的一种重要的算法，LARS相比起传统的Forward selection和Forward stagewise，既不那么富于侵略性，又比较走捷径。LARS算法在lasso 估计的求解中也有非常好的应用。在Efron等人的同篇论文中有详细的讨论。关于lasso和它的LARS算法，笔者将在今后的文章中介绍。
