---
title: 教程----什么是变分自动编码器
author: 
  - 姚杰
  - 黄理强
date: '2018-06-27'
meta_extra: "原作者：Jaan Altosaar；译者：姚杰，黄理强"
categories:
  - 机器学习
tags:
  - 机器学习
  - 翻译
forum_id: ""
---

> 本文翻译自[Jaan Altosaar的博客](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)，本文已获得原作者授权。

> 从深度学习和图模型这两个角度来理解变分自动编码器(Variational Autoencoders，VAE)。

为什么深度学习研究者和概率机器学习人员谈及变分自动编码器会感到困惑？什么是变分自动编码器？围绕这个名词为什么会有一些不合理的疑惑？

这里存在着概念与语义的差异。神经网络与概率模型的学科领域之间没有共同的语言。我的目标是弥合这种思想差距，并且激发两个领域之间更多的合作和讨论，同时提供一个一致的实现。如果这里有很多词汇对你来说是新的词汇，可以跳到术语表。

变分自动编码器很酷。它可以使得我们基于数据设计复杂的生成模型，并且在大数据集上拟合。它可以生成虚构的名人脸部图像以及高分辨率的数字艺术作品。

![由变分自动编码器生成的虚构名人脸(作者: ALEC RADFORD)](https://jaan.io/images/variational-autoencoder-faces.jpg)

<center>由变分自动编码器生成的虚构名人脸[作者: ALEC RADFORD](https://www.youtube.com/watch?v=XNZIN7Jh3Sg) </center>

这些模型同时在图像生成和强化学习中也能产生最先进的机器学习结果。Kingma和Rezenda等人在2013年定义了变分自动编码器(VAEs)。

我们如何创建一种讨论变分自动编码器的语言？让我们首先使用神经网络来思考他们，然后在概率模型中使用变分推断。

# 神经网络角度

在神经网络语言中，变分自动编码器由编码器，解码器和损失函数组成。

![编码器将数据压缩成一个潜在空间(Z)，解码器在给定隐藏表示的情况下重建数据](https://jaan.io/images/encoder-decoder.png)

<center>编码器将数据压缩成一个潜在空间(Z)，解码器在给定隐藏表示的情况下重建数据</center>

该编码器是一个神经网络。它的输入是一个数据点`$x$`，它的输出是一个隐藏表示`$z$`，它有权重和偏差`$\theta $`。具体来说，让我们假设x是28*28像素大小的手写数字照片。编码器将784维的数据压缩成一个远远小于784维的隐藏表示空间z。这通常被称为“瓶颈”，因为编码器必须学习将数据有效压缩到这个较低维空间。让我们来表示这个编码器`$q_{\theta}(z∣x) $`。我们注意到，低维空间是随机的：编码器将参数输出给`$q_{\theta}(z∣x)$`，该公式本身是一个高斯概率密度。我们可以从这个分布中抽样来获得表示z的嘈杂值。

解码器是另一个神经网络。它的输入是隐藏表示z，它的输出是数据的概率分布的参数，并且具有权重和偏差`$\phi $`。解码器用 `$p_{\phi}(x∣z)$`表示。运行手写数字示例，照片只有黑白两色，并将每个像素表示成0或1。可以使用伯努利分布来表示单个像素的概率分布。解码器的输入是数字的隐藏表示z，输出是图像中784像素对应的784个伯努利参数。解码器将z中的实数解码成0-1之间的784个实数。由于信息从较小维度转变成较大维度，信息存在丢失。那么有多少信息丢失呢？我们利用重新构造的对数似然函数 `$p_{\phi}(x∣z)$`来衡量数据丢失，其单位为奈特。这个度量告诉我们解码器基于隐藏表示z是如何有效的重构输入图像x。

变分自动编码器的损失函数是带有正则化的负对数似然的期望。因为不存在全部样本点可以共用的全局表示，我们可以将损失函数分解为仅依赖于单个数据点的项`$l_{i}$`。总损失是N个数据点的总损失`$\sum^N_{i=1} l_{i}$`。对于数据点xi的损失函数是：
`$$l_{i}(\theta,\phi)=−E_{z∼q_{\theta}(z∣x_{i})}[logp_{\phi}(x_{i}∣z)]+KL(q_{\theta}(z∣x_{i})∣∣p(z)) $$`
第一项是重构损失，或者称为对于第i个数据点的负对数似然性的期望。期望是关于编码器在表示z上的分布。这一项鼓励着解码器学习重建数据。如果解码器的输出不能很好的重建数据，它会导致损失函数产生一个很大的代价。

第二项是我们引入的正则项(在之后我们将看待它是如何派生的)。这是编码器分布`$q_{\theta}(z∣x)$` 和`$p(z)$`之间的Kullback-Leibler差异，简称K-L散度。K-L散度衡量了使用q来代表p时，丢失了多少信息（以纳特为单位）。这是一种衡量q与p接近程度的指标。

在变分自动编码器中，p指定为均值为0，方差为1的标准正态分布。`$p(z)=Normal(0,1)$`如果编码器的输出z与标准正太分布不同，它将在损失函数中收到惩罚。这个正则项意味着“每个数字的隐藏表示空间z保持足够多样化”。如果我们不包括正则项，编码器会学会欺骗并在欧几里得空间的不同区域给每个数据点一个表示。这是不对的，因为这会使得相同数字的两张图片(比如不同人写的数字2)最终会得到两个很不同的表示z。我们想要的表示空间z是有意义的，因此我们惩罚这种行为。正则项使得相似数字的表示空间存在相似性(比如不同人写的数字2的表示空间保持相似)。

我们使用梯度下降训练变分自动编码器，优化关于编码器的参数 `$\theta $`和解码器的参数 `$\phi $`的损失函数。对于随机梯度下降的步长`$\rho $`，编码器用 `$\theta←\theta−\rho\frac{\partial\theta}{\partial l} $`更新参数，解码器也是类似的。
# 概率模型角度

现在让我们从概率模型的角度思考变分自动编码器。请忘掉你所了解的有关深度学习和神经网络的一切。脱离神经网络考虑以下概念将更为清晰，最后我们再重新回到神经网络。

在概率模型框架中，变分自动编码器包含数据`$x$`的概率模型以及潜变量`$z$`。我们可以将模型的联合概率写成 `$p(x,z)=p(x∣z)p(z)$` 。生成过程如下。
对于每个数据点i:
  - 绘制潜在变量：`$z_{i}∼p(z)$`
  - 绘制数据点：`$x_{i}∼p(x∣z)$`

我们可以将其表示为一个图模型：

![变分自动编码器中模型的图模型表示](https://jaan.io/images/graphical-model-variational-autoencoder.png)

<center>变分自动编码器中模型的图模型表示。潜变量Z是一个标准正态变量，数据是根据`$P(X|Z)$`绘制的。X的阴影节点表示观测数据。对于手写数字的黑白图像，其似然是服从伯努利分布。</center>

这是我们在从概率模型的角度讨论变分自动编码器时考虑的中心对象。潜在变量来自于先验概率`$p(z)$`。数据`$x$`有基于潜在变量z的类条件概率`$p(x|z)$`。该模型定义了数据和潜在变量的联合概率分布：`$p(x,z)$`。我们可以将其分解为条件概率和先验概率：`$p(x,z)=p(x|z)p(z)$`。对于黑白的数字，条件概率服从伯努利分布。

现在我们可以在这个模型中考虑推理。我们的目标是基于观测数据给出的潜在变量的，或者计算后验概率`$p(z∣x)$`。贝叶斯公式如下：
`$$p(z|x) = p(x|z)\frac{p(x∣z)}{p(z)}$$`.
查看分母`$p(x)$`。这是全概率，我们可以通过潜在变量计算：`$p(x)=\int p(x∣z)p(z)dz$`。不幸的是，这个积分需要大量的计算时间，因为它需要考虑潜在变量的所有可能情况。因此我们需要近似后验概率。

变分推断近似于分布簇 `$q_{\lambda}(z∣x)$` 的后验概率。变分参数`$\lambda $`代表分布簇。例如：如果`$q$`是高斯分布，对于每个数据点的潜在变量的均值和方差变为 `$\lambda_{x_{i}} =(\mu_{x_{i}},\sigma_{x_{i}}{^2}))$`。

我们怎么能知道我们变换的后验概率`$q(z∣x)$`是否近似于真正的后验概率`$p(z∣x)$`？我们可以使用K-L散度，它测量用q近似p的过程中丢失的信息(其单位为奈特)：
`$$KL(q_{\lambda}(z∣x)∣∣p(z∣x))=\mathbf{E}_{q}[\log q_{\lambda}(z\vert x)]-\mathbf{E}_{q}[\log p(x, z)] + \log p(x)Eq [logq_\lambda (z∣x)]−Eq logp(x,z)]+logp(x)$$`
我们的目标是找到能使K-L散度最小的变分参数`$ \lambda $`。因此最佳近似后验概率是：
`$$ q_{\lambda}^{∗}(z∣x)=argmin_{\lambda}KL(q_{\lambda}(z∣x)∣∣p(z∣x))$$`
这为什么不能直接计算？因为令人讨厌的p(x)出现在K-L散度中。如上所述，这是棘手的。我们需要一个新的变量来进行易于理解的变分推断。考虑到以下的函数：
`$$ELBO(\lambda)=E_{q}[logp(x,z)]−E_{q}[logq_{\lambda}(z∣x)]$$`
我们可以将这个函数跟K-L散度结合起来，并将函数重写为：
`$$\log p(x) = ELBO(\lambda) + KL(q_{\lambda}(z \vert x) \vert \vert p(z \vert x))$$`
根据Jensen不等式，K-L散度总是大于或等于零。这意味着最小化K-L散度等同于最大化ELBO。ELBO使得我们能够做近似后验概率。我们不必计算和最小化近似后验概率和实际的后验概率的K-L散度。相反，我们可以最大化ELBO来实现同样的作用(同时在计算上也更容易实现)。

在变分自动编码器模型中，只有局部的潜在变量(数据点的隐藏表示`$z$`与其他数据点的隐藏表示之间不存在共享)。所以我们可以将ELBO分解为一个总和，其中每项依赖于一个数据点。这使得我们可以针对参数使用随机梯度下降`$\lambda $`(重要的是：变化的参数在数据点之间共享)变分自动编码器中单个数据点的ELBO是：
`$$ELBO_{i}(\lambda)=E_{q\lambda(z∣x_{i})}[logp(x_{i}∣z)]−KL(q_\lambda(z∣x_{i})∣∣p(z)).$$`
为了能理解这个公式与之前对ELBO的定义相同，将对数项转换成先验概率和条件概率，并将乘积规则运用于对数项。

让我们联系到神经网络概念，最后一步是为近似后验概率`$q_{\theta}(z∣x,\lambda) $`调优参数，通过推理网络将输入数据`$x$`生成相应的输出参数`$\lambda $`。我们参数化带有生成网络的条件概率`$p(x∣z) $`，它获取潜在变量并将参数输出到数据分布`$p_{\phi}(x∣z) $`。推理和生成网络有各自的参数`$\theta $`  和`$\phi $`。参数通常是神经网络的权重和偏差。我们通过随机梯度下降优化这些参数，从而获得最大化ELBO(没有全局潜在变量，因此小批量处理我们的数据是可以的)。我们可以编写ELBO并将推理和生成网络参数包括为：
`$$ELBO_{i}(\theta, \phi) = E_{q_{\theta}(z\vert x_{i})}[\log p_{\phi}(x_{i}\vert z)] - KL(q_{\theta}(z\vert x_{i})\vert\vertp(z))。 $$`
这个函数的下限是我们从神经网络角度讨论的变分自动编码器的损失函数的负值:
`$ELBO_{i}(\theta,\phi)=−l_{i}(\theta,\phi) $`。但是我们是从概率模型和近似后验估计的公式推理中得到了它。我们仍然可以将K-L散度项解释为正规化，并将预期的条件概率项解释为重建“损失”。但概率模型方法明确了这些条件存在的原因：为了最小化近似后验概率`$q_{\lambda}(z∣x) $`和模型后验概率`$p(z∣x) $`之间的K-L散度。

模型参数怎么样呢？我们掩饰了这一点，但是这也是很关键的。术语“变分推断”通常是指关于变分参数`$\lambda $`的ELBO的最大化。我们也可以根据模型参数`$\phi $`使得ELBO最大化(例如生成神经网络的权重和偏差)。这个技术称为变分EM(期望最大化)，因为我们正在基于模型参数最大化数据的对数条件概率的期望。

我们已经遵循变分推断的方法，我们已经定义：
  - 具有潜在变量和数据的概率模型p
  - 为了潜在变量近似后验概率的变量簇q

然后我们使用变分推断算法来学习变分参数(在ELBO上使用梯度上升来学习`$\lambda $`)。我们使用变分EM作为模型参数(在ELBO上使用梯度上升来学习`$\phi $`)

# 实验

现在我们已经准备好查看模型中的样本了。 我们有两种选择来衡量进步: 先验概率和后验概率。 为了让我们更好地理解潜空间，我们可以想象潜变量`$q_{\lambda}(z∣x) $` 的后验分布是怎样的。

在计算上，这意味着通过推理网络输入图像x得到正态分布的参数，然后取一个潜变量z的样本。我们可以在训练期间将其绘图，以了解推理网络如何学习以更好地逼近后验分布，并将潜变量分配给潜空间的不同部分标上不同类别的数字。请注意，在训练开始时，潜变量的分布接近先验分布(0周围的一个圆斑)。

<iframe src="https://giphy.com/embed/26ufoVqZDjHoPrp8k" width="480" height="414" frameBorder="0" class="giphy-embed" allowFullScreen></iframe>

将训练过程中逼近后验概率的学习进行可视化。随着训练的进行，数字分类在二维空间中变得分化。

我们也可以将先验预测分布可视化。我们将潜变量固定为-3和3之间的等间隔的值，然后我们可以从生成网络参数化的条件概率中抽取样本。这些“幻觉”图像向我们展示了模型与潜空间的每个部分的联系。

<iframe src="https://giphy.com/embed/26ufgj5LH3YKO1Zlu" width="480" height="480" frameBorder="0" class="giphy-embed" allowFullScreen></iframe>

通过观察条件概率的样本，将预测分布可视化。X轴和 Y轴表示二维中-3和3之间等距的潜变量值。

# 术语表

我们需要以一种清晰而简洁的方式来决定讨论变分自动编码器的语言。以下是我发现令人困惑的术语：

变分自动编码器(VAE)：在神经网络语言中，VAE由编码器，解码器和损失函数组成。在概率模型术语中，变分自编码器是指隐性高斯模型中的近似推理，其近似后验和条件概率模型由神经网络(推理和生成网络)参数化得到。

损失函数(Loss function)：在神经网络语言中，我们考虑损失函数。训练意味着尽量减少这些损失函数。但在变分推断中，我们最大化ELBO(这不是损失函数)。这会导致像调用名为 `optimizer。minimize(-elbo)` 的优化器那样的尴尬，因为神经网络框架中的优化器只支持最小化。

编码器(Encoder)：在神经网络世界中，编码器是根据数据x输出z的神经网络。在概率模型术语中，推理网络将潜变量z的近似后验参数化。推理网络将参数输出到分布`$q(z|x) $`。

解码器(Decoder)：在深度学习中，解码器是根据z学习重建数据x的神经网络。就概率模型而言，给定潜变量z时数据x的条件概率由生成网络进行参数化。生成网络将参数输出到似然分布`$p(x|z) $`。

局部潜变量(Local Latent Variables)：这些是每个数据点xi的对应的`$z_{i}$`。全局潜变量是不存在的。因为只有局部潜变量，我们可以轻松地将ELBO分解为只依赖于单个数据点`$x_{i}$`的`$L_{i}$`，这使得随机梯度下降得以实现。

推理(Inference)：在神经网络中，推理通常意味着给出新的，从未见过的数据点的隐藏表示预测。在概率模型中，推理是指根据给定观测数据推断潜变量的值。

一个充满行话的概念应该有它自己的小节: 

## 平均场推理vs均摊推理

这个问题让我非常困惑 ，对于具有深度学习背景的人，这可能会更令人困惑。在深度学习中，我们考虑输入和输出，编码器和解码器以及损失函数。在学习概率建模时，这会导致模糊，不精确的概念。

我们来讨论平均场推理与均摊推理的不同之处。这是我们在估计潜变量的后验概率时所面临的一个选择 。我们可能有各种限制：我们有很多数据吗？我们有大型计算机或者图形处理器？我们是否有局部的每个数据点潜变量，或所有数据点共享的全局潜在变量？

平均场变分推断是一种对N个数据点进行因式分解的变分分布的选择，它没有共享参数：
`$$q(z) = \prod_{i}^{N} q(z_{i}; \lambda_{i}) $$`
这意味着每个数据点`$\lambda_{i} $`都有自由参数(例如高斯潜变量`$\lambda_{i}=(\mu_{i},\sigma_{i}) $`)。我们如何为一个新的，不可见的数据点“学习”？对于每个数据点，我们需要对其平均字段参数`$\lambda_{i}$`将ELBO最大化。

摊销推理是指“均摊”数据点之间的推理成本。一种做法是通过在数据点之间共享(均摊)变分参数`$ \lambda $`来实现。例如，在变分自动编码器中，推理网络的参数`$ \theta $`。这些全局参数在所有数据点之间共享。如果我们看到一个新的数据点，并想看看它的近似后验`$q(z_{i}) $`的样子,我们可以再次进行变分推断(最大化ELBO直到收敛)，或者相信共享参数已经“足够好”。这可能是一个优于平均场的地方。

哪一个更灵活？平均场推理具有更严格的表达力，因为它没有共享参数。每个数据参数`$\lambda_{i} $`可以确保我们的近似后验是最忠实于数据的。另一种思考方式是，我们通过将参数绑定到数据点(例如，在数据上共享权重和偏差的神经网络)来限制变分家族的容量或表达力。

## 样本实现

这里有一个简单的实现，用来生成这篇文章中的数字: [Github 链接](https://github.com/altosaar/vae/blob/master/vae.py) 

## 脚注: 再参数化的技巧

实现变分自动编码器所需要的最后一件事情是如何在随机变量的参数上进行求导。 如果给出了从分布`$q_{\theta}(z∣x) $`中得到的`$z$`，而且我们希望对`$z$`的函数求关于`$\theta $`的导数，我们应该怎样做？`$z$`的样本是固定的，但是直觉上它的导数应该非零。

对于某些分布，可以用一种巧妙的方式再参数化样本，这样的抽象性是独立于参数外的。我们希望我们的样本在确定性上取决于分布的参数。例如，在一个平均值为`$\mu $`和标准差为`$\sigma $`的正态分布中，我们可以这样取样：
`$$z = \mu + \sigma \odot \epsilon $$`
其中`$\epsilon \sim Normal(0,1) $`，由表示从分布中抽取的`$∼$`到表示相等的`$=$`是关键的一步。我们已经定义了一个函数，它依赖于参数的确定。因此，我们可以对`$z$`，`$f(z) $`的函数求关于它的分布参数`$\mu $`和`$\sigma $`的导数。
![img](https://jaan.io/images/reparametrization.png)
再参数化技巧允许我们将一个正态分布的随机变量Z的随机性推入`$\epsilon $`，这是从标准正态中取样的。菱形表示确定的依赖关系，圆圈表示随机变量

在变分自动编码器中，平均值和方差是由具有我们优化的参数`$\theta $`推理网络输出的。再参数化的技巧使得我们可以通过作为潜变量`$z$`的样本的函数的目标(ELBO)对`$\theta $`反向传播(链式求导)。

(作者原话)如果这篇文章中的任何内容令你感到困惑，或者有什么改进的建议？请提交[pull request](https://github.com/altosaar/jaan.io/blob/master/_posts/blog/2016-07-18-what-is-variational-autoencoder-vae-tutorial.md), [tweet](https://twitter.com/thejaan), 或者 [email](mailto:altosaar@princeton.edu) 

## 想法和图解的参考

许多想法和图解来自Shakir Mohamed关于[再参数化技巧](http://blog.shakirm。com/2015/10/machine-learning-trick-of-the-day-4-reparameterisation-tricks/)和 [自动编码器](http://blog.shakirm.com/2015/03/a-statistical-view-of-deep-learning-ii-auto-encoders-and-free-energy/)的优秀博客文章。Durk Kingma创造了[再参数化技巧](http://dpkingma.com/?page_id=277)优秀的视觉效果。这个 [教程](https://arxiv.org/abs/1601.00670)和David Blei’s的 [课程笔记](https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf)对于变分推断有很好的参考。Dustin Tran有有一篇关于变分自动编码器的有用的 [博客文章](http://dustintran.com/blog/denoising-criterion-for-variational-auto-encoding-framework/)。开头的动图来自于[Rui Shu](https://github.com/RuiShu/variational-autoencoder)。

感谢 Rajesh Ranganath，Ben Poole，Jon Berliner，Cassandra Xia，and Ryan Sepassi 在本文中的讨论和提出的许多概念。

关于[Hacker News](https://news.ycombinator.com/edit?id=12292576)和 [Reddit](https://wwwreddit.com/r/MachineLearning/comments/4xv5b5/explainer_of_variational_autoencoders_from_a/)的讨论。 在 David Duvenaud 关于"[可微推理和生成模型](http://www.cs.toronto.edu/~duvenaud/courses/csc2541/)"的课程大纲中。
```

```
