---
title: 为什么用户不爱钱：一篇学术论文的诞生记
date: '2021-04-30'
author: "冯璟烁"
slug: why-user-do-not-like-money-a-tdm-research
categories:
  - R语言
tags:
  - R 语言
  - TDM
---

> 全文共计3869字，预计需要10分钟。
此文讲述一篇最近发表在 Transportation Research Part C <sup>[1]</sup> 期刊上的论文背后的故事。这篇方法论的论文始于一个实际数据问题的启发，在这里想和大家分享一下自己从发现问题到最终写成论文的历程。

首先，来思考这样一个问题：

今天，在你出门上班前，有人提出愿意给你1元钱，但与之相对的，需要你推迟10分钟出门，你愿不愿意？

不用着急回答，我们先来谈谈为什么“有人”要给你钱？

## 故事的起因

交通拥堵一直是困扰城市建设者们的一个问题，交通需求管理 [Transportation Demand Management, TDM] 相关的研究想要通过各种鼓励或管控的方法缓解这个问题。许多交通管控手段都属于 TDM 的范畴，诸如车辆限行、建设公交系统、征收拥堵费等。

![“全球最堵”洛杉矶在自己的电影里也免不了被调侃一下（La La Land）](https://user-images.githubusercontent.com/37477330/116766377-c1332600-aa5c-11eb-998e-b9a60200b667.jpg)

最近几年，一些 TDM 研究将目光瞄准了人们的手机——通过利用手机 app 尝试进行个性化的需求管理。例如，对于同样是早上 9 点出门的用户，传统的 TDM 因为无法直接接触每个具体用户，只能借助通用的政策来改变人们的行为，效果往往不如预期。这是因为每个人面临的挑战都不一样，比如免费公交或公交减价的政策可能只对少数价格十分敏感的人有用，而这些人则很可能已经在挤公交了。而随着智能手机的广泛使用，”个性化”的需求管理则可以做到推荐一些用户提前出门，推荐一些用户推迟出门，或是给一部分用户推荐其他可行路线等等差异化的出行方案。如此一来，可以让道路资源从繁忙的时间段向相对空闲的时间段转移，从路网中压力最大的道路向一些相对闲置的道路转移，帮助整个城市实现错峰出行，减少拥堵。错峰管理可以和现行的种种管理方法如公交系统、拥堵费等一起执行，以期取得最佳效果。

![把需求分散出去：20 个早上 9 点出门的用户](https://user-images.githubusercontent.com/37477330/116766379-c3958000-aa5c-11eb-87dd-c09cc709f649.png)

当然由于每个人也都有自己的难处，所以交通需求管理 [TDM] 需要设计鼓励机制，比如，公交系统的低票价，高昂的城区停车费，高峰期的拥堵费等就是通过充分拉开在高峰期开车与不开车的成本差距，让人们尽量放弃高峰期在城区开车的想法。个性化 TDM 系统则更进一步通过向具体用户提供特定的奖励，来鼓励用户改变原计划而按照推荐的备选方案出行。于是，也就有了文章开头的问题。

## “奇怪”的用户

**那么我们需要给用户多少钱？**

对于用户而言，奖励的钱越多，应该会更愿意做出改变。但对于服务整体居民的 TDM 系统，给出的每一分钱都是成本，如何经济有效的给备选方案定价，是一个十分重要的问题。

华盛顿大学的 THINK LAB 实验室最近做过一个实验<sup>[2]</sup>。实验共征集了1840 名受访者，让其使用一个模拟的个性化 TDM 系统。用户在输入行程信息之后，系统会生成一个个性化的备选方案供用户选择，新方案会要求用户提前出门或是推迟出门，并说明会节省的路途时间。同时为了鼓励用户改变，系统会对新方案分配一定量的积分奖励，如果用户执行了新方案就会获得对应的奖励。每次奖励的数额会根据备选方案设置、用户特征、交互历史而有所不同，并会显示在选择界面上。在这个实验中，受访者会进行 多次这样的选择，以帮助我们了解用户的喜好。

![实验截图：一个选择的例子](https://user-images.githubusercontent.com/37477330/116766384-c98b6100-aa5c-11eb-80ef-ed23a7cdfb30.png)

我们的合作者首先采用了效用最大化准则 [Random Utility Maximization, RUM theory]建立混合 Logit 模型 [Mixed Logit Model]。对于不了解 RUM<sup>[3]</sup> 的同学，简而言之，RUM 准则认为，当人们面对选择时，会评估每个选项的效用 [utility]，并从中选择最高的一个。这里的效用 [utility]，是指一种针对选项吸引力的价值判断：用户根据自己对选项中各个特征的喜好，综合评价这一选项有多吸引他或是能带来多少价值，效应越高就月有吸引力，也就越容易让人接纳。比如说在这个数据集中：根据备选方案中对提早出门 $x_1$、推迟出门 $x_2$、节省路途时间 $x_3$、积分奖励 $x_4$ 四个变量的值，以及人们对于这些改变的喜好 $(β_1,β_2,β_3,β_4)$，基于 RUM 的模型就可以计算出效用，比如：


$$
Utility=β_0+β_1 x_1+β_2 x_2+β_3 x_3+β_4 x_4+ϵ
$$
![效用最大化准则[RUM]的模型示意图](https://user-images.githubusercontent.com/37477330/116766385-cb552480-aa5c-11eb-940f-ba2692fa3bbf.png)

但是在分析这个实验收集的数据后，RUM 模型的结果却出乎意料——训练出的模型系数显示有超过20%的受访者的节省路途时间的系数为负，更有40%的受访者积分奖励的系数为负。换言之，给钱越多，却越没吸引力？

## 一个新的解释

为什么 RUM 模型无法解释这个实验中的用户行为？

一个模型的诞生，有它的历史背景，了解这个历史背景，也会有助于我们了解它的历史局限性。所以，我们对比了一下 RUM 模型常用的数据情景和我们当下碰到的情况。我们发现，RUM 模型适用的情景是一个经典统计拟合问题：有一个理论上存在的样本，然后我们在现实中抽样，然后通过拟合数据去了解那个看不见的样本。也就是说，在这种经典设定下，数据收集的过程与收集到的数据是独立的。但在我们的实验里，我们的数据是通过一种与用户交互的新型方式收集起来的。也就是说，用户的行为也许不是一个静态目标，而是一个动态过程。因此，RUM 模型也许并不适合这种情景。

让我们重新思考一下我们要建模的问题，也就是文章开头提到的那个问题：

**给你1 元钱，但与之相对的，需要你推迟 10 分钟出门，你愿不愿意？**

在判断是否接受这个提议时，我们——作为用户——是怎么做出决定的？

当面对一个推荐的备选方案时，如果决定接受，意味着我们需要为此做出一些“牺牲”（改变原有出行计划），同时赚取一些“奖励”。如果拒绝，则用户继续按照自己的出行方案，也就没有奖励。根据这些想法，我们提出了一个新的模型来刻画用户的决策：潜在决策阈值模型 [Latent Decision Threshold model, LDT model]。这个模型有两个基本假设：

1. 系统给出的每一种出行方案，根据其具体内容（比如提前多早出门，节省多少时间），会对用户构成一个成本。而这个系统给出的出行方案离用户原有的方案差距越大，用户接受这个方案的成本就越高。
2. 只有当积分奖励大于这个用户的成本的情况下，用户才有可能接受这个系统推荐的出行方案。

因此就有了下图中的模型。

![潜在决策阈值模型[LDT]的示意图](https://user-images.githubusercontent.com/37477330/116766386-cdb77e80-aa5c-11eb-8710-79444c441f30.png)

比较 LDT 模型与 RUM 模型，我们可以发现一点：假如 LDT 模型是对的，那么上图所示的 RUM 模型必然会存在共线性的问题，因为这意味着积分奖励这个变量跟其他变量是有关的。而事实上，在 THINK LAB 的实验里，在系统提供备选方案时，定价确实常常和几个变量有一定关系。而经历过的朋友们应该会有所体会，如果在回归模型中出现共线性问题，模型的参数估计本身是不稳定的，很多结论也会匪夷所思。而 LDT 模型则不存在这个问题。

## 潜在决策阈值模型 [LDT model]

有了这个结构后，我们继而写出了这个潜在决策阈值模型的数学表达以及模型的训练方法，然后发现 LDT 模型的训练过程可以从支持向量机 [Support Vector Machine, SVM] 中得到启发。

首先，这个模型的“比较”机制涉及到两个值——我们对于推荐的备选方案的预估价，即接受的成本，也就是所谓潜在决策阈值 [Latent Decision Threshold, LDT]；以及备选方案可以提供的奖励 $x_4$。为了避免我们下意识将这几个本质上不同的变量像在回归模型里面一样排成一列，我们从现在开始，给它一个独立的字母 $r$ [reward]来代替 $x_4$，而把 LDT 假设为一个线性函数，$f(x_1,x_2,x_3 )=α_0+α_1 x_1+α_2 x_2+α_3 x_3$。当 $r>LDT$ 时，我们接受这个推荐（意味着收益大于预估价），模型中标记为 $y=1$，否则，就不接受，模型中用 $y=-1$ 来表示拒绝。这样一来，我们就可以将这个两层结构的模型表示在一个统一的公式中，从而可以方便我们之后的模型训练。

$$
y(r-f(x_1,x_2,x_3 ))≥0
$$

据此推导出的潜在决策阈值模型 [LDT model] 就转化为：找到一个超平面，将两组点y分隔开。显然，实际应用中线性可分的情况非常少，我们需要允许一些点被分错。所以类似 SVM，我们也采用软边界 [soft margin] 以及最大边界 [max margin] 的方法，放宽一些要求并且让边界尽量远离从而保证模型在测试集上的效果。于是，便得到如下训练方案：


$$
\begin{aligned}
\min \qquad_{\alpha_{0}, \alpha, \xi} \frac{1}{2} \boldsymbol{\alpha}^{\top} \boldsymbol{\alpha}+C \sum_{t} \xi_{t} \\
\text { s.t. }  y_{t}\left(r_{t}-\alpha_{0}-\boldsymbol{\alpha}^{\top} \boldsymbol{x}_{t}\right) \geq 1-\xi_{t}, \text { for all } t, \\
 \xi_{t} \geq 0, \text { for all } t,
\end{aligned}
$$

![软边界的支持向量机[Soft margin SVM]](https://user-images.githubusercontent.com/37477330/116766389-cf814200-aa5c-11eb-89d5-940b137f5473.png)

## 解释 LDT

有了模型，有了解法，如何评估我们的新模型？又要如何说明确实比之前的 RUM 解释的更好呢？

回到 THINK LAB 的实验数据。之前的RUM模型显示很多用户“不喜欢省时间”以及“不喜欢奖励。我们将——用户喜欢节省路途时间，喜欢获得奖励——视为“易于解释”，系数如果相反则视为“难以解释”。

我们举个例子：用户 ID 2260 号，RUM 模型的系数是：$(β_0,β_1,β_2,β_3,β_4 )=(26.117,0.183,-4.452,-0.361,-0.085)$。根据  RUM 理论，备选方案中的提前出门增加 1 分钟，可以提高这个选项的效用为 0.183，而推迟出门，节省时间，以及积分奖励的增加，都会降低选项的效用。每单位变化将带来的降低量分别为4.452，0.361，0.085. 换言之，模型告诉我们，给备选方案提供更多的奖励积分不仅不能让选项变得更具吸引力，相反，这名用户更加不愿意选择这个选项了。也就是我们所说的，出现了难以解释的系数。

而对于用户 ID 2260 号，LDT 的系数分别为：$(β_0,β_1,β_2,β_3 )=(38.659,-0.937,0.312,-0.156)$。LDT 模型并没有积分奖励相关的系数，因为在 LDT 模型里，积分奖励是和潜在阈值进行比较的，而比较的结果直接决定了用户的决策。因此在 LDT 模型中，我们对积分奖励的作用与其他变量的作用分别建了模。另一方面，我们的模型建立在这个前提上：提高积分奖励的影响是积极的，因为用户会更有可能选择我们提供的备选方案。除了积分奖励之外，这个用户的路途节省时间的系数值为-0.156，也比之前 RUM 更好解释。在这里，多节省 1分钟，将意味着用户的决策阈值会降低 0.156，也就是说，在能省时间的情况下，为了让用户改变出行行为，所需要提供的积分奖励也更少（0.156）。

可以看到，类似于 ID 2260 号这样的用户，在新的 LDT 模型下，“反常”系数没有了，解释也就更好理解了。

另外，因为在 LDT 模型中，系数的值直接对应奖励积分，截距项也变得更好解释。$β_0=38.659$ 可以解释为，在被要求改变出行行为时，这名用户至少需要 38.659 分的“启动”积分才愿意改变其习惯（最终分数也和其他变量有关），这也符合我们的常规认知。

从这个个例出发，我们总结RUM 模型以及新的 LDT 模型的估计结果如下表。从这个角度来看，LDT 模型的结果难以解释的用户比例比 RUM 要少。我们也可以看到，LDT 模型仍然也会出现一批“反常”用户。而事实上，我们深入这些反常用户的具体数据的时候，确实发现他们的行为反常，比如无区别拒绝（或者接受）所有选项，拒绝高回报选项选择低回报选项，等等。考虑到 THINK 实验室给予参与这个实验的参与者的现金奖励，我们相信这些用户并未如实的参与这个实验，也就是说，这些用户属于投机者， LDT 模型的结果恰恰反映了这一点。

|                          |       RUM     |        LDT    |
| ------------------  |------------------  |
|$β_3$，节省时间  | 21.25% | 13.15% |
|$β_4$，积分奖励 | 38.84% | 0 |

## 结语

如果你对这篇文章感兴趣，可以移步 https://doi.org/10.1016/j.trc.2020.102814.十分欢迎大家踊跃拍砖讨论！

## 参考文献

[1] Feng, J., Huang, S., & Chen, C. (2020). Modeling user interaction with app-based reward system: A graphical model approach integrated with max-margin learning. Transportation Research Part C: Emerging Technologies, 120, 102814.

[2] Zhu, X., Wang, F., Chen, C., & Reed, D. D. (2020). Personalized incentives for promoting sustainable travel behaviors. Transportation Research Part C: Emerging Technologies, 113, 314-331.

[3] https://www.nobelprize.org/uploads/2018/06/mcfadden-lecture.pdf
