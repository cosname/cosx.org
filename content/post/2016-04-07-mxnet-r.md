---
title: mxnet：结合R与GPU加速深度学习
date: '2016-04-07T10:13:38+00:00'
author: 严酷的魔王
categories:
  - 统计之都
tags:
  - boosting
  - dmlc
  - R语言
  - xgboost
  - 深度学习
slug: mxnet-r
---

近年来，深度学习可谓是机器学习方向的明星概念，不同的模型分别在图像处理与自然语言处理等任务中取得了前所未有的好成绩。在实际的应用中，大家除了关心模型的准确度，还常常希望能比较快速地完成模型的训练。一个常用的加速手段便是将模型放在GPU上进行训练。然而由于种种原因，R语言似乎缺少一个能够在GPU上训练深度学习模型的程序包。

![](https://raw.githubusercontent.com/dmlc/dmlc.github.io/master/img/logo-m/mxnetR.png)

DMLC（Distributed (Deep) Machine Learning Community）是由一群极客发起的组织，主要目标是提供快速高质量的开源机器学习工具。近来流行的boosting模型xgboost便是出自这个组织。最近DMLC开源了一个深度学习工具mxnet，这个工具含有R，python，julia等语言的接口。本文以R接口为主，向大家介绍这个工具的性能与使用方法。

# 一、五分钟入门指南

在这一节里，我们在一个样例数据上介绍mxnet的基本使用方法。目前mxnet还没有登录CRAN的计划，所以安装方法要稍微复杂一些。

  * 如果你是Windows/Mac用户，那么可以通过下面的代码安装预编译的版本。这个版本会每周进行预编译，不过为了保证兼容性，只能使用CPU训练模型。

    install.packages(<span class="pl-s"><span class="pl-pds">"</span>drat<span class="pl-pds">"</span></span>, <span class="pl-v">repos</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>https://cran.rstudio.com<span class="pl-pds">"</span></span>)
    <span class="pl-e">drat</span><span class="pl-k">:::</span>addRepo(<span class="pl-s"><span class="pl-pds">"</span>dmlc<span class="pl-pds">"</span></span>)
    install.packages(<span class="pl-s"><span class="pl-pds">"</span>mxnet<span class="pl-pds">"</span></span>)

  * 如果你是Linux用户或者想尝试GPU版本，请参考[这个链接里](http://mxnet.io/get_started/setup.html)的详细编译教程在本地进行编译。

安装完毕之后，我们就可以开始训练模型了，下面两个小节分别介绍两种不同的训练神经网络的方法。

### 二分类模型与mx.mlp

首先，我们准备一份数据，并进行简单的预处理：

    require(mlbench)
    require(mxnet)
    data(Sonar, package="mlbench")
    Sonar[,61] = as.numeric(Sonar[,61])-1
    train.ind = c(1:50, 100:150)
    train.x = data.matrix(Sonar[train.ind, 1:60])
    train.y = Sonar[train.ind, 61]
    test.x = data.matrix(Sonar[-train.ind, 1:60])
    test.y = Sonar[-train.ind, 61]

我们借用`mlbench`包中的一个二分类数据，并且将它分成训练集和测试集。`mxnet`提供了一个训练多层神经网络的函数`mx.mlp`，我们额可以通过它来训练一个神经网络模型。下面是`mx.mlp`中的部分参数：

  * 训练数据与预测变量
  * 每个隐藏层的大小
  * 输出层的结点数
  * 激活函数类型
  * 损失函数类型
  * 进行训练的硬件（CPU还是GPU）
  * 其他传给mx.model.FeedForward.create的高级参数

了解了大致参数后，我们就可以理解并让R运行下面的代码进行训练了。

    mx.set.seed<span class="p">(</span><span class="m">0</span><span class="p">)</span>
    model <span class="o"><-</span> mx.mlp<span class="p">(</span>train.x<span class="p">,</span> train.y<span class="p">,</span> hidden_node<span class="o">=</span><span class="m">10</span><span class="p">,</span> out_node<span class="o">=</span><span class="m">2</span><span class="p">,</span>      out_activation<span class="o">=</span><span class="s">"softmax"</span><span class="p">, </span>num.round<span class="o">=</span><span class="m">20</span><span class="p">,</span> array.batch.size<span class="o">=</span><span class="m">15</span><span class="p">,</span> learning.rate<span class="o">=</span><span class="m">0.07</span><span class="p">,</span> momentum<span class="o">=</span><span class="m">0.9</span><span class="p">,</span> eval.metric<span class="o">=</span>mx.metric.accuracy<span class="p">)</span>

    ## Auto detect layout of input matrix, use rowmajor..
    ## Start training with 1 devices
    ## [1] Train-accuracy=0.488888888888889
    ## [2] Train-accuracy=0.514285714285714
    ## [3] Train-accuracy=0.514285714285714
    
    ...
    
    ## [18] Train-accuracy=0.838095238095238
    ## [19] Train-accuracy=0.838095238095238
    ## [20] Train-accuracy=0.838095238095238

这里要注意使用mx.set.seed而不是R自带的set.seed函数来控制随机数。因为mxnet的训练过程可能会运行在不同的运算硬件上，我们需要一个足够快的随机数生成器来管理整个随机数生成的过程。模型训练好之后，我们可以很简单地进行预测：

<div class="highlight">
  <pre><code class="language-r">preds &lt;span class="o">=&lt;/span> predict&lt;span class="p">(&lt;/span>model&lt;span class="p">,&lt;/span> test.x&lt;span class="p">)&lt;/span>
</code></pre>
</div>

<div class="highlight">
  <pre><code class="language-text">## Auto detect layout of input matrix, use rowmajor..
</code></pre>
</div>

<div class="highlight">
  <pre><code class="language-r">pred.label &lt;span class="o">=&lt;/span> &lt;span class="kp">max.col&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kp">t&lt;/span>&lt;span class="p">(&lt;/span>preds&lt;span class="p">))&lt;/span>&lt;span class="m">-1&lt;/span>
&lt;span class="kp">table&lt;/span>&lt;span class="p">(&lt;/span>pred.label&lt;span class="p">,&lt;/span> test.y&lt;span class="p">)&lt;/span>
</code></pre>
</div>

<div class="highlight">
  <pre><code class="language-text">##           test.y
## pred.label  0  1
##          0 24 14
##          1 36 33</code></pre>
</div>

如果进行的是多分类预测，mxnet的输出格式是类数X样本数。

### 回归模型与自定义神经网络

mx.mlp接口固然很方便，但是神经网络的一大特点便是它的灵活性，不同的结构可能有着完全不同的特性。mxnet的亮点之一便是它赋予了用户极大的自由度，从而可以任意定义需要的神经网络结构。我们在这一节用一个简单的回归任务介绍相关的语法。

首先，我们仍然要准备好一份数据。

<pre><code class="language-r">data&lt;span class="p">(&lt;/span>BostonHousing&lt;span class="p">,&lt;/span> package&lt;span class="o">=&lt;/span>&lt;span class="s">"mlbench"&lt;/span>&lt;span class="p">)&lt;/span>

train.ind &lt;span class="o">=&lt;/span> &lt;span class="kp">seq&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">506&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">3&lt;/span>&lt;span class="p">)&lt;/span>
train.x &lt;span class="o">=&lt;/span> &lt;span class="kp">data.matrix&lt;/span>&lt;span class="p">(&lt;/span>BostonHousing&lt;span class="p">[&lt;/span>train.ind&lt;span class="p">,&lt;/span> &lt;span class="m">-14&lt;/span>&lt;span class="p">])&lt;/span>
train.y &lt;span class="o">=&lt;/span> BostonHousing&lt;span class="p">[&lt;/span>train.ind&lt;span class="p">,&lt;/span> &lt;span class="m">14&lt;/span>&lt;span class="p">]&lt;/span>
test.x &lt;span class="o">=&lt;/span> &lt;span class="kp">data.matrix&lt;/span>&lt;span class="p">(&lt;/span>BostonHousing&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>train.ind&lt;span class="p">,&lt;/span> &lt;span class="m">-14&lt;/span>&lt;span class="p">])&lt;/span>
test.y &lt;span class="o">=&lt;/span> BostonHousing&lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>train.ind&lt;span class="p">,&lt;/span> &lt;span class="m">14&lt;/span>&lt;span class="p">]&lt;/span></code></pre>

mxnet提供了一个叫做“Symbol”的系统，从而使我们可以定义结点之间的连接方式与激活函数等参数。下面是一个定义没有隐藏层神经网络的简单例子：

<pre><code class="language-r">&lt;span class="c1"># 定义输入数据&lt;/span>
data &lt;span class="o">&lt;-&lt;/span> mx.symbol.Variable&lt;span class="p">(&lt;/span>&lt;span class="s">"data"&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="c1"># 完整连接的隐藏层&lt;/span>
&lt;span class="c1"># data: 输入源&lt;/span>
&lt;span class="c1"># num_hidden: 该层的节点数&lt;/span>
fc1 &lt;span class="o">&lt;-&lt;/span> mx.symbol.FullyConnected&lt;span class="p">(&lt;/span>data&lt;span class="p">,&lt;/span> num_hidden&lt;span class="o">=&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="p">)&lt;/span>

&lt;span class="c1"># 针对回归任务，定义损失函数&lt;/span>
lro &lt;span class="o">&lt;-&lt;/span> mx.symbol.LinearRegressionOutput&lt;span class="p">(&lt;/span>fc1&lt;span class="p">)&lt;/span></code></pre>

在神经网络中，回归与分类的差别主要在于输出层的损失函数。这里我们使用了平方误差来训练模型。希望能更进一步了解Symbol的读者可以继续阅读这份以[代码](http://mxnet.io/tutorials/r/symbol.html)为主的文档。

定义了神经网络之后，我们便可以使用mx.model.FeedForward.create进行训练了。

<div class="highlight">
  <pre><code class="language-r">mx.set.seed&lt;span class="p">(&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">)&lt;/span>
model &lt;span class="o">&lt;-&lt;/span> mx.model.FeedForward.create&lt;span class="p">(&lt;/span>lro&lt;span class="p">,&lt;/span> X&lt;span class="o">=&lt;/span>train.x&lt;span class="p">,&lt;/span> y&lt;span class="o">=&lt;/span>train.y&lt;span class="p">, &lt;/span>ctx&lt;span class="o">=&lt;/span>mx.cpu&lt;span class="p">(),&lt;/span> num.round&lt;span class="o">=&lt;/span>&lt;span class="m">50&lt;/span>&lt;span class="p">,&lt;/span> array.batch.size&lt;span class="o">=&lt;/span>&lt;span class="m">20&lt;/span>&lt;span class="p">, &lt;/span>learning.rate&lt;span class="o">=&lt;/span>&lt;span class="m">2e-6&lt;/span>&lt;span class="p">,&lt;/span> momentum&lt;span class="o">=&lt;/span>&lt;span class="m">0.9&lt;/span>&lt;span class="p">,&lt;/span> eval.metric&lt;span class="o">=&lt;/span>mx.metric.rmse&lt;span class="p">)&lt;/span>
</code></pre>
</div>

<div class="highlight">
  <pre><code class="language-text">## Auto detect layout of input matrix, use rowmajor..
## Start training with 1 devices
## [1] Train-rmse=16.063282524034
## [2] Train-rmse=12.2792375712573
## [3] Train-rmse=11.1984634005885

...

## [48] Train-rmse=8.26890902770415
## [49] Train-rmse=8.25728089053853
## [50] Train-rmse=8.24580511500735</code></pre>
</div>

这里我们还针对回归任务修改了eval.metric参数。目前我们提供的评价函数包括“accuracy”，“rmse”，“mae” 和 “rmsle”，用户也可以针对需要自定义评价函数，例如：

<pre><code class="language-r">demo.metric.mae &lt;span class="o">&lt;-&lt;/span> mx.metric.custom&lt;span class="p">(&lt;/span>&lt;span class="s">"mae"&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kr">function&lt;/span>&lt;span class="p">(&lt;/span>label&lt;span class="p">,&lt;/span> pred&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
  res &lt;span class="o">&lt;-&lt;/span> &lt;span class="kp">mean&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kp">abs&lt;/span>&lt;span class="p">(&lt;/span>label&lt;span class="o">-&lt;/span>pred&lt;span class="p">))&lt;/span>
  &lt;span class="kr">return&lt;/span>&lt;span class="p">(&lt;/span>res&lt;span class="p">)&lt;/span>
&lt;span class="p">})&lt;/span></code></pre>

<div class="highlight">
  <pre><code class="language-r">mx.set.seed&lt;span class="p">(&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">)&lt;/span>
model &lt;span class="o">&lt;-&lt;/span> mx.model.FeedForward.create&lt;span class="p">(&lt;/span>lro&lt;span class="p">,&lt;/span> X&lt;span class="o">=&lt;/span>train.x&lt;span class="p">,&lt;/span> y&lt;span class="o">=&lt;/span>train.y&lt;span class="p">, &lt;/span>ctx&lt;span class="o">=&lt;/span>mx.cpu&lt;span class="p">(),&lt;/span> num.round&lt;span class="o">=&lt;/span>&lt;span class="m">50&lt;/span>&lt;span class="p">,&lt;/span> array.batch.size&lt;span class="o">=&lt;/span>&lt;span class="m">20&lt;/span>&lt;span class="p">, &lt;/span>learning.rate&lt;span class="o">=&lt;/span>&lt;span class="m">2e-6&lt;/span>&lt;span class="p">,&lt;/span> momentum&lt;span class="o">=&lt;/span>&lt;span class="m">0.9&lt;/span>&lt;span class="p">,&lt;/span> eval.metric&lt;span class="o">=&lt;/span>demo.metric.mae&lt;span class="p">)&lt;/span>
</code></pre>
</div>

<div class="highlight">
  <pre><code class="language-text">## Auto detect layout of input matrix, use rowmajor..
## Start training with 1 devices
## [1] Train-mae=13.1889538083225
## [2] Train-mae=9.81431959337658
## [3] Train-mae=9.21576419870059

...

## [48] Train-mae=6.41731406417158
## [49] Train-mae=6.41011292926139
## [50] Train-mae=6.40312503493494</code></pre>
</div>

至此，你已经掌握了基本的mxnet使用方法。接下来，我们将介绍更好玩的应用。

# 二、手写数字竞赛

在这一节里，我们以Kaggle上的[手写数字数据集（MNIST）竞赛](https://www.kaggle.com/c/digit-recognizer)为例子，介绍如何通过mxnet定义一个强大的神经网络，并在GPU上快速训练模型。

第一步，我们从Kaggle上[下载数据](https://www.kaggle.com/c/digit-recognizer/data)，并将它们放入data/文件夹中。然后我们读入数据，并做一些预处理工作。

<div class="highlight">
  <pre><code class="language-r">&lt;span class="kn">require&lt;/span>&lt;span class="p">(&lt;/span>mxnet&lt;span class="p">)&lt;/span></code>
train <span class="o">&lt;-</span> read.csv<span class="p">(</span><span class="s">'data/train.csv'</span><span class="p">,</span> header<span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span> 
test <span class="o">&lt;-</span> read.csv<span class="p">(</span><span class="s">'data/test.csv'</span><span class="p">,</span> header<span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span> 
train <span class="o">&lt;-</span> <span class="kp">data.matrix</span><span class="p">(</span>train<span class="p">)</span> 
test <span class="o">&lt;-</span> <span class="kp">data.matrix</span><span class="p">(</span>test<span class="p">)</span> 

train.x <span class="o">&lt;-</span> train<span class="p">[,</span><span class="m">-1</span><span class="p">]</span> 
train.y <span class="o">&lt;-</span> train<span class="p">[,</span><span class="m">1</span><span class="p">]

</span><code class="language-r">train.x &lt;span class="o">&lt;-&lt;/span> &lt;span class="kp">t&lt;/span>&lt;span class="p">(&lt;/span>train.x&lt;span class="o">/&lt;/span>&lt;span class="m">255&lt;/span>&lt;span class="p">)&lt;/span>
test &lt;span class="o">&lt;-&lt;/span> &lt;span class="kp">t&lt;/span>&lt;span class="p">(&lt;/span>test&lt;span class="o">/&lt;/span>&lt;span class="m">255&lt;/span>&lt;span class="p">)&lt;/span></code></pre>
</div>

最后两行预处理的作用有两个：

  * 原始灰度图片数值处在[0,255]之间，我们将其变换到[0,1]之间。
  * mxnet接受 像素X图片 的输入格式，所以我们对输入矩阵进行了转置。

接下来我们定义一个特别的神经网络结构：[LeNet](http://yann.lecun.com/exdb/lenet/)。这是Yann LeCun提出用于识别手写数字的结构，也是最早的卷积神经网络之一。同样的，我们使用Symbol语法来定义，不过这次结构会比较复杂。

<pre><code class="language-r">&lt;span class="c1"># input&lt;/span>
data &lt;span class="o">&lt;-&lt;/span> mx.symbol.Variable&lt;span class="p">(&lt;/span>&lt;span class="s">'data'&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="c1"># first conv&lt;/span>
conv1 &lt;span class="o">&lt;-&lt;/span> mx.symbol.Convolution&lt;span class="p">(&lt;/span>data&lt;span class="o">=&lt;/span>data&lt;span class="p">,&lt;/span> kernel&lt;span class="o">=&lt;/span>&lt;span class="kt">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">5&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">5&lt;/span>&lt;span class="p">),&lt;/span> num_filter&lt;span class="o">=&lt;/span>&lt;span class="m">20&lt;/span>&lt;span class="p">)&lt;/span>
tanh1 &lt;span class="o">&lt;-&lt;/span> mx.symbol.Activation&lt;span class="p">(&lt;/span>data&lt;span class="o">=&lt;/span>conv1&lt;span class="p">,&lt;/span> act_type&lt;span class="o">=&lt;/span>&lt;span class="s">"tanh"&lt;/span>&lt;span class="p">)&lt;/span>
pool1 &lt;span class="o">&lt;-&lt;/span> mx.symbol.Pooling&lt;span class="p">(&lt;/span>data&lt;span class="o">=&lt;/span>tanh1&lt;span class="p">,&lt;/span> pool_type&lt;span class="o">=&lt;/span>&lt;span class="s">"max"&lt;/span>&lt;span class="p">,&lt;/span>
                          kernel&lt;span class="o">=&lt;/span>&lt;span class="kt">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">2&lt;/span>&lt;span class="p">),&lt;/span> stride&lt;span class="o">=&lt;/span>&lt;span class="kt">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">2&lt;/span>&lt;span class="p">))&lt;/span>
&lt;span class="c1"># second conv&lt;/span>
conv2 &lt;span class="o">&lt;-&lt;/span> mx.symbol.Convolution&lt;span class="p">(&lt;/span>data&lt;span class="o">=&lt;/span>pool1&lt;span class="p">,&lt;/span> kernel&lt;span class="o">=&lt;/span>&lt;span class="kt">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">5&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">5&lt;/span>&lt;span class="p">),&lt;/span> num_filter&lt;span class="o">=&lt;/span>&lt;span class="m">50&lt;/span>&lt;span class="p">)&lt;/span>
tanh2 &lt;span class="o">&lt;-&lt;/span> mx.symbol.Activation&lt;span class="p">(&lt;/span>data&lt;span class="o">=&lt;/span>conv2&lt;span class="p">,&lt;/span> act_type&lt;span class="o">=&lt;/span>&lt;span class="s">"tanh"&lt;/span>&lt;span class="p">)&lt;/span>
pool2 &lt;span class="o">&lt;-&lt;/span> mx.symbol.Pooling&lt;span class="p">(&lt;/span>data&lt;span class="o">=&lt;/span>tanh2&lt;span class="p">,&lt;/span> pool_type&lt;span class="o">=&lt;/span>&lt;span class="s">"max"&lt;/span>&lt;span class="p">,&lt;/span>
                          kernel&lt;span class="o">=&lt;/span>&lt;span class="kt">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">2&lt;/span>&lt;span class="p">),&lt;/span> stride&lt;span class="o">=&lt;/span>&lt;span class="kt">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="m">2&lt;/span>&lt;span class="p">))&lt;/span>
&lt;span class="c1"># first fullc&lt;/span>
flatten &lt;span class="o">&lt;-&lt;/span> mx.symbol.Flatten&lt;span class="p">(&lt;/span>data&lt;span class="o">=&lt;/span>pool2&lt;span class="p">)&lt;/span>
fc1 &lt;span class="o">&lt;-&lt;/span> mx.symbol.FullyConnected&lt;span class="p">(&lt;/span>data&lt;span class="o">=&lt;/span>flatten&lt;span class="p">,&lt;/span> num_hidden&lt;span class="o">=&lt;/span>&lt;span class="m">500&lt;/span>&lt;span class="p">)&lt;/span>
tanh3 &lt;span class="o">&lt;-&lt;/span> mx.symbol.Activation&lt;span class="p">(&lt;/span>data&lt;span class="o">=&lt;/span>fc1&lt;span class="p">,&lt;/span> act_type&lt;span class="o">=&lt;/span>&lt;span class="s">"tanh"&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="c1"># second fullc&lt;/span>
fc2 &lt;span class="o">&lt;-&lt;/span> mx.symbol.FullyConnected&lt;span class="p">(&lt;/span>data&lt;span class="o">=&lt;/span>tanh3&lt;span class="p">,&lt;/span> num_hidden&lt;span class="o">=&lt;/span>&lt;span class="m">10&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="c1"># loss&lt;/span>
lenet &lt;span class="o">&lt;-&lt;/span> mx.symbol.SoftmaxOutput&lt;span class="p">(&lt;/span>data&lt;span class="o">=&lt;/span>fc2&lt;span class="p">)&lt;/span></code></pre>

为了让输入数据的格式能对应LeNet，我们要将数据变成R中的array格式：

<pre><code class="language-r">train.array &lt;span class="o">&lt;-&lt;/span> train.x
&lt;span class="kp">dim&lt;/span>&lt;span class="p">(&lt;/span>train.array&lt;span class="p">)&lt;/span> &lt;span class="o">&lt;-&lt;/span> &lt;span class="kt">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">28&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">28&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kp">ncol&lt;/span>&lt;span class="p">(&lt;/span>train.x&lt;span class="p">))&lt;/span>
test.array &lt;span class="o">&lt;-&lt;/span> test
&lt;span class="kp">dim&lt;/span>&lt;span class="p">(&lt;/span>test.array&lt;span class="p">)&lt;/span> &lt;span class="o">&lt;-&lt;/span> &lt;span class="kt">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">28&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">28&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kp">ncol&lt;/span>&lt;span class="p">(&lt;/span>test&lt;span class="p">))&lt;/span></code></pre>

接下来我们将要分别使用CPU和GPU来训练这个模型，从而展现不同的训练效率。

<pre><code class="language-r">n.gpu &lt;span class="o">&lt;-&lt;/span> &lt;span class="m">1&lt;/span>
device.cpu &lt;span class="o">&lt;-&lt;/span> mx.cpu&lt;span class="p">()&lt;/span>
device.gpu &lt;span class="o">&lt;-&lt;/span> &lt;span class="kp">lapply&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="o">:&lt;/span>&lt;span class="p">(&lt;/span>n.gpu&lt;span class="m">-1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="kr">function&lt;/span>&lt;span class="p">(&lt;/span>i&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
  mx.gpu&lt;span class="p">(&lt;/span>i&lt;span class="p">)&lt;/span>
&lt;span class="p">})&lt;/span></code></pre>

我们可以将GPU的每个核以list的格式传递进去，如果有BLAS等自带矩阵运算并行的库存在，则没必要对CPU这么做了。

我们先在CPU上进行训练，这次我们只进行一次迭代：

<div class="highlight">
  <pre><code class="language-r">mx.set.seed&lt;span class="p">(&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">)&lt;/span>
tic &lt;span class="o">&lt;-&lt;/span> &lt;span class="kp">proc.time&lt;/span>&lt;span class="p">()&lt;/span>
model &lt;span class="o">&lt;-&lt;/span> mx.model.FeedForward.create&lt;span class="p">(&lt;/span>lenet&lt;span class="p">,&lt;/span> X&lt;span class="o">=&lt;/span>train.array&lt;span class="p">,&lt;/span> y&lt;span class="o">=&lt;/span>train.y&lt;span class="p">, &lt;/span>ctx&lt;span class="o">=&lt;/span>device.cpu&lt;span class="p">,&lt;/span> num.round&lt;span class="o">=&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="p">,&lt;/span> array.batch.size&lt;span class="o">=&lt;/span>&lt;span class="m">100&lt;/span>&lt;span class="p">, &lt;/span>learning.rate&lt;span class="o">=&lt;/span>&lt;span class="m">0.05&lt;/span>&lt;span class="p">,&lt;/span> momentum&lt;span class="o">=&lt;/span>&lt;span class="m">0.9&lt;/span>&lt;span class="p">,&lt;/span> wd&lt;span class="o">=&lt;/span>&lt;span class="m">0.00001&lt;/span>&lt;span class="p">, &lt;/span>eval.metric&lt;span class="o">=&lt;/span>mx.metric.accuracy&lt;span class="p">, &lt;/span>epoch.end.callback&lt;span class="o">=&lt;/span>mx.callback.log.train.metric&lt;span class="p">(&lt;/span>&lt;span class="m">100&lt;/span>&lt;span class="p">))&lt;/span>
</code></pre>
</div>

<div class="highlight">
  <pre><code class="language-text">## Start training with 1 devices
## Batch [100] Train-accuracy=0.1066
## Batch [200] Train-accuracy=0.16495
## Batch [300] Train-accuracy=0.401766666666667
## Batch [400] Train-accuracy=0.537675
## [1] Train-accuracy=0.557136038186157
</code></pre>
</div>

<div class="highlight">
  <pre><code class="language-r">&lt;span class="kp">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kp">proc.time&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">-&lt;/span> tic&lt;span class="p">)&lt;/span>
</code></pre>
</div>

<div class="highlight">
  <pre><code class="language-text">##    user  system elapsed
## 130.030 204.976  83.821</code></pre>
</div>

在CPU上训练一次迭代一共花了83秒。接下来我们在GPU上训练5次迭代：

<div class="highlight">
  <pre><code class="language-r">mx.set.seed&lt;span class="p">(&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="p">)&lt;/span>
tic &lt;span class="o">&lt;-&lt;/span> &lt;span class="kp">proc.time&lt;/span>&lt;span class="p">()&lt;/span>
model &lt;span class="o">&lt;-&lt;/span> mx.model.FeedForward.create&lt;span class="p">(&lt;/span>lenet&lt;span class="p">,&lt;/span> X&lt;span class="o">=&lt;/span>train.array&lt;span class="p">,&lt;/span> y&lt;span class="o">=&lt;/span>train.y&lt;span class="p">, &lt;/span>ctx&lt;span class="o">=&lt;/span>device.gpu&lt;span class="p">,&lt;/span> num.round&lt;span class="o">=&lt;/span>&lt;span class="m">5&lt;/span>&lt;span class="p">,&lt;/span> array.batch.size&lt;span class="o">=&lt;/span>&lt;span class="m">100&lt;/span>&lt;span class="p">, &lt;/span>learning.rate&lt;span class="o">=&lt;/span>&lt;span class="m">0.05&lt;/span>&lt;span class="p">,&lt;/span> momentum&lt;span class="o">=&lt;/span>&lt;span class="m">0.9&lt;/span>&lt;span class="p">,&lt;/span> wd&lt;span class="o">=&lt;/span>&lt;span class="m">0.00001&lt;/span>&lt;span class="p">, &lt;/span>eval.metric&lt;span class="o">=&lt;/span>mx.metric.accuracy&lt;span class="p">, &lt;/span>epoch.end.callback&lt;span class="o">=&lt;/span>mx.callback.log.train.metric&lt;span class="p">(&lt;/span>&lt;span class="m">100&lt;/span>&lt;span class="p">))&lt;/span>
</code></pre>
</div>

<div class="highlight">
  <pre><code class="language-text">## Start training with 1 devices
## Batch [100] Train-accuracy=0.1066
## Batch [200] Train-accuracy=0.1596
## Batch [300] Train-accuracy=0.3983
## Batch [400] Train-accuracy=0.533975
## [1] Train-accuracy=0.553532219570405
## Batch [100] Train-accuracy=0.958
## Batch [200] Train-accuracy=0.96155
## Batch [300] Train-accuracy=0.966100000000001
## Batch [400] Train-accuracy=0.968550000000003
## [2] Train-accuracy=0.969071428571432
## Batch [100] Train-accuracy=0.977
## Batch [200] Train-accuracy=0.97715
## Batch [300] Train-accuracy=0.979566666666668
## Batch [400] Train-accuracy=0.980900000000003
## [3] Train-accuracy=0.981309523809527
## Batch [100] Train-accuracy=0.9853
## Batch [200] Train-accuracy=0.985899999999999
## Batch [300] Train-accuracy=0.986966666666668
## Batch [400] Train-accuracy=0.988150000000002
## [4] Train-accuracy=0.988452380952384
## Batch [100] Train-accuracy=0.990199999999999
## Batch [200] Train-accuracy=0.98995
## Batch [300] Train-accuracy=0.990600000000001
## Batch [400] Train-accuracy=0.991325000000002
## [5] Train-accuracy=0.991523809523812
</code></pre>
</div>

<div class="highlight">
  <pre><code class="language-r">&lt;span class="kp">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kp">proc.time&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">-&lt;/span> tic&lt;span class="p">)&lt;/span>
</code></pre>
</div>

<div class="highlight">
  <pre><code class="language-text">##    user  system elapsed
##   9.288   1.680   6.889</code></pre>
</div>

在GPU上训练5轮迭代只花了不到7秒，快了数十倍！可以看出，对于这样的网络结构，GPU的加速效果是非常显著的。有了快速训练的办法，我们便可以很快的做预测，并且提交到Kaggle上了：

<pre><code class="language-r">preds &lt;span class="o">&lt;-&lt;/span> predict&lt;span class="p">(&lt;/span>model&lt;span class="p">,&lt;/span> test.array&lt;span class="p">)&lt;/span>
pred.label &lt;span class="o">&lt;-&lt;/span> &lt;span class="kp">max.col&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kp">t&lt;/span>&lt;span class="p">(&lt;/span>preds&lt;span class="p">))&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="m">1&lt;/span>
submission &lt;span class="o">&lt;-&lt;/span> &lt;span class="kt">data.frame&lt;/span>&lt;span class="p">(&lt;/span>ImageId&lt;span class="o">=&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="o">:&lt;/span>&lt;span class="kp">ncol&lt;/span>&lt;span class="p">(&lt;/span>test&lt;span class="p">),&lt;/span> Label&lt;span class="o">=&lt;/span>pred.label&lt;span class="p">)&lt;/span>
write.csv&lt;span class="p">(&lt;/span>submission&lt;span class="p">,&lt;/span> file&lt;span class="o">=&lt;/span>&lt;span class="s">'submission.csv'&lt;/span>&lt;span class="p">,&lt;/span> row.names&lt;span class="o">=&lt;/span>&lt;span class="kc">FALSE&lt;/span>&lt;span class="p">,&lt;/span> quote&lt;span class="o">=&lt;/span>&lt;span class="kc">FALSE&lt;/span>&lt;span class="p">)&lt;/span></code></pre>

# 三、图像识别应用

其实对于神经网络当前的应用场景而言，识别手写数字已经不够看了。早些时候，Google公开了一个云API，让用户能够检测一幅图像里面的内容。现在我们提供一个教程，让大家能够自制一个图像识别的在线应用。

DMLC用在ImageNet数据集上训练了一个模型，能够直接拿来对真实图片进行分类。同时，我们搭建了一个Shiny应用，只需要不超过150行R代码就能够自己在浏览器中进行图像中的物体识别。

为了搭建这个应用，我们要安装shiny和imager两个R包：

<pre><code class="language-r">install.packages("shiny", repos="https://cran.rstudio.com")
install.packages("imager", repos="https://cran.rstudio.com")</code></pre>

现在你已经配置好了mxnet, shiny和imager三个R包，最困难的部分已经完成了！下一步则是让shiny直接下载并运行我们准备好的代码：

<pre><code class="language-r">shiny::runGitHub("thirdwing/mxnet_shiny")</code></pre>

第一次运行这个命令会花上几分钟时间下载预先训练好的模型。训练的模型是Inception-BatchNorm Network，如果读者对它感兴趣，可以阅读这篇文章。准备就绪之后，你的浏览器中会出现一个网页应用，就用本地或在线图片来挑战它吧！

如果你只需要一个图像识别的模块，那么我们下面给出最简单的一段R代码让你能进行图像识别。首先，我们要导入预训练过的模型文件：

<pre><code class="language-r">model &lt;span class="o">&lt;&lt;-&lt;/span> mx.model.load&lt;span class="p">(&lt;/span>&lt;span class="s">"Inception/Inception_BN"&lt;/span>&lt;span class="p">,&lt;/span> iteration &lt;span class="o">=&lt;/span> &lt;span class="m">39&lt;/span>&lt;span class="p">)&lt;/span>
synsets &lt;span class="o">&lt;&lt;-&lt;/span> &lt;span class="kp">readLines&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">"Inception/synset.txt"&lt;/span>&lt;span class="p">)&lt;/span>
mean.img &lt;span class="o">&lt;&lt;-&lt;/span> &lt;span class="kp">as.array&lt;/span>&lt;span class="p">(&lt;/span>mx.nd.load&lt;span class="p">(&lt;/span>&lt;span class="s">"Inception/mean_224.nd"&lt;/span>&lt;span class="p">)[[&lt;/span>&lt;span class="s">"mean_img"&lt;/span>&lt;span class="p">]])&lt;/span></code></pre>

接下来我们使用一个函数对图像进行预处理，这个步骤对于神经网络模型而言至关重要。

<pre><code class="language-r">preproc.image &lt;span class="o">&lt;-&lt;/span> &lt;span class="kr">function&lt;/span>&lt;span class="p">(&lt;/span>im&lt;span class="p">,&lt;/span> mean.image&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
  &lt;span class="c1"># crop the image&lt;/span>
  shape &lt;span class="o">&lt;-&lt;/span> &lt;span class="kp">dim&lt;/span>&lt;span class="p">(&lt;/span>im&lt;span class="p">)&lt;/span>
  short.edge &lt;span class="o">&lt;-&lt;/span> &lt;span class="kp">min&lt;/span>&lt;span class="p">(&lt;/span>shape&lt;span class="p">[&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="o">:&lt;/span>&lt;span class="m">2&lt;/span>&lt;span class="p">])&lt;/span>
  yy &lt;span class="o">&lt;-&lt;/span> &lt;span class="kp">floor&lt;/span>&lt;span class="p">((&lt;/span>shape&lt;span class="p">[&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">-&lt;/span> short.edge&lt;span class="p">)&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="m">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="m">1&lt;/span>
  yend &lt;span class="o">&lt;-&lt;/span> yy &lt;span class="o">+&lt;/span> short.edge &lt;span class="o">-&lt;/span> &lt;span class="m">1&lt;/span>
  xx &lt;span class="o">&lt;-&lt;/span> &lt;span class="kp">floor&lt;/span>&lt;span class="p">((&lt;/span>shape&lt;span class="p">[&lt;/span>&lt;span class="m">2&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">-&lt;/span> short.edge&lt;span class="p">)&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="m">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="m">1&lt;/span>
  xend &lt;span class="o">&lt;-&lt;/span> xx &lt;span class="o">+&lt;/span> short.edge &lt;span class="o">-&lt;/span> &lt;span class="m">1&lt;/span>
  croped &lt;span class="o">&lt;-&lt;/span> im&lt;span class="p">[&lt;/span>yy&lt;span class="o">:&lt;/span>yend&lt;span class="p">,&lt;/span> xx&lt;span class="o">:&lt;/span>xend&lt;span class="p">,,]&lt;/span>
  &lt;span class="c1"># resize to 224 x 224, needed by input of the model.&lt;/span>
  resized &lt;span class="o">&lt;-&lt;/span> resize&lt;span class="p">(&lt;/span>croped&lt;span class="p">,&lt;/span> &lt;span class="m">224&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">224&lt;/span>&lt;span class="p">)&lt;/span>
  &lt;span class="c1"># convert to array (x, y, channel)&lt;/span>
  arr &lt;span class="o">&lt;-&lt;/span> &lt;span class="kp">as.array&lt;/span>&lt;span class="p">(&lt;/span>resized&lt;span class="p">)&lt;/span>
  &lt;span class="kp">dim&lt;/span>&lt;span class="p">(&lt;/span>arr&lt;span class="p">)&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kt">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">224&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">224&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">3&lt;/span>&lt;span class="p">)&lt;/span>
  &lt;span class="c1"># substract the mean&lt;/span>
  normed &lt;span class="o">&lt;-&lt;/span> arr &lt;span class="o">-&lt;/span> mean.img
  &lt;span class="c1"># Reshape to format needed by mxnet (width, height, channel, num)&lt;/span>
  &lt;span class="kp">dim&lt;/span>&lt;span class="p">(&lt;/span>normed&lt;span class="p">)&lt;/span> &lt;span class="o">&lt;-&lt;/span> &lt;span class="kt">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="m">224&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">224&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="p">)&lt;/span>
  &lt;span class="kr">return&lt;/span>&lt;span class="p">(&lt;/span>normed&lt;span class="p">)&lt;/span>
&lt;span class="p">}&lt;/span></code></pre>

最后我们读入图像，预处理与预测就可以了。

<pre><code class="language-r">im &lt;span class="o">&lt;-&lt;/span> load.image&lt;span class="p">(&lt;/span>src&lt;span class="p">)&lt;/span>
normed &lt;span class="o">&lt;-&lt;/span> preproc.image&lt;span class="p">(&lt;/span>im&lt;span class="p">,&lt;/span> mean.img&lt;span class="p">)&lt;/span>
prob &lt;span class="o">&lt;-&lt;/span> predict&lt;span class="p">(&lt;/span>model&lt;span class="p">,&lt;/span> X &lt;span class="o">=&lt;/span> normed&lt;span class="p">)&lt;/span>
max.idx &lt;span class="o">&lt;-&lt;/span> &lt;span class="kp">order&lt;/span>&lt;span class="p">(&lt;/span>prob&lt;span class="p">[,&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="p">],&lt;/span> decreasing &lt;span class="o">=&lt;/span> &lt;span class="kc">TRUE&lt;/span>&lt;span class="p">)[&lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="o">:&lt;/span>&lt;span class="m">5&lt;/span>&lt;span class="p">]&lt;/span>
result &lt;span class="o">&lt;-&lt;/span> synsets&lt;span class="p">[&lt;/span>max.idx&lt;span class="p">]&lt;/span></code></pre>

# 四、参考资料

MXNet是一个在底层与接口都有着丰富功能的软件，如果读者对它感兴趣，可以参考一些额外的材料来进一步了解MXNet，或者是深度学习这个领域。

  * [MXNet on github](https://github.com/dmlc/mxnet)
  * [MXNet完整文档](http://mxnet.readthedocs.org/en/latest/)
  * [mxnet R包入门文档](http://mxnet.io/tutorials/index.html#r-tutorials)
  * [结合Shiny+MXNet搭建在线识图服务](http://dmlc.ml/rstats/2015/12/08/image-classification-shiny-app-mxnet-r.html)
  * [深度学习入门](http://ufldl.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B)
  * [DMLC主页](http://dmlc.ml/)

&nbsp;
