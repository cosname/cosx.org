---
title: "浅谈随机事件与变量"
date: '2022-12-03'
author: "权恒恒"
categories: 
- 统计学
- 统计应用
tags: 
- 概率定义
- 随机变量
- 随机事件
slug: random variable
---

首先，不可否认的是，现代数学是建立在集合理论的基础之上的，比如我们熟知的定义域，线性空间等都是集合，这也包括近世代数中群、环、域以及拓扑学的拓扑空间等。可以说集合理论是现代数学的基石。在概率论中也不例外，因此在许多有关概率论的书籍中，集合及其运算通常会放在最靠前的部分。

另一方面，数学的研究从建模开始，为现实世界建立数学模型，通常是数学工作者的第一步。对概率论来说，概率论的产生源于结果的不确定性，而这种不确定性是数学家不愿看到的，为了能够有效描述这一不确定现象，由此引申出了随机事件这一概念。

我们把所有可能的结果看成是一个**集合**，称之为样本空间，集合中的每一个元素叫做样本点，集合的子集叫做随机事件，在约定俗成中，通常用大写字母A，B，C来表示随机事件。比方说，抛一个骰子，那么所有可能的结果为1，2，3，4，5，6，因此我们把所有可能的结果看成是一个集合，不妨记为

`$$\Omega = \{1,2,3,4,5,6\}$$`

集合的子集有

`$$\{1\},\{3\},\{6\},\{1,3\},\{1,3,5\},\ldots$$`

事件A表示抛出点数为6的概率，那么子集`$\{6\}$`就可以表示随机事件A。这里我们强调了从集合的观点来看待随机事件，以及用集合来表示随机事件的方法。

如果随机事件A为抛一个骰子两次，点数和为7的事件，那么对应的所有可能结果的集合需要用一对数来表示，前者表示第一次，后者表示第二次，即有

`$$\Omega = \{(1,1),(1,2),(1,3),(2,3),(4,5),\ldots\}$$`

集合的子集有

`$$\{(1,1)\},\{(1,3)\},\{(1,2),(1,3)\},\{(1,3),(1,4),(2,5)\},\ldots$$`

那么事件A可表示为几个子集之并，即

`$$\{(1,6)\}\cup \{(2,5)\}\cup \{(3,4)\}\cup \{(4,3)\}\cup \{(5,2)\} \cup \{(6,1)\}$$`

此外，我们还可以发现，事件A也可以用下面的子集来直接表示而不用取并集，即

`$$\{(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)\}$$`

从上面的例子我们现在发现了有些随机事件可以有多种子集的表示方式，而有些则不行。这也说明了对现实世界的建模，角度不同，方法也略有差异。

我们再考虑一个随机事件，张三的公司实行弹性制，张三上班通常在7:00-9:00，他上班从来不迟到，但大部分情况下他在8:00左右到公司。李四和张三在同一家公司，但李四会迟到，就是9:00以后到，但偶尔也会提前，比如7:00之前到，那么也就说这一天他可能在任何时刻来公司。

针对上面的叙述，我们选定一个随机事件进行讨论，考虑一个随机事件A，就是张三和李四两个人8:00到公司的概率。

我们分析来看，张三所对应的所有可能结果的样本空间为

`$$\Omega = \{T|7:00\leq T \leq 9:00\}$$`

李四对应的样本空间为

`$$\Omega = \{T|0:00\leq T \leq 24:00\}$$`


需要说明的是，前面抛骰子的例子，所有可能的结果是可数的，因此我们采用列举法来表示集合，在这个例子中，如果我们精确到秒，集合的元素也是可数的，如果我们足够精细，那么就不可数了，因此这里我们采用的是描述法来描述集合。

事件A对应的子集为

`$$\{T=8:00\}$$`

集合中有有限个元素和集合中有无穷的元素的分析方法是不一样的，这里我们只是粗略的看，先不做定量的计算。

随机事件五花八门，千奇百怪，人们为了便于研究和分析，将随机事件映射到实数上，或者说是数轴上，从而形成了随机变量的概念。这在概率论上实现了一次飞跃。补充说一下，“映射”其实就是一种函数，当然，严格来说，函数是映射的一种，映射的涵义比函数范围要广。这也就是在概率测度空间中，随机变量为什么又称为可测函数。

这里列举几个典型的例子。

示例1，比如抛一个硬币，结果就两个，正和反，我们可以用数轴上的0表示正，1表示反，也可以用-1表示反，1表示正，也可以用5表示反，2022表示正，总之，只要不引起歧义，用数轴上的两个点即可表示。

示例2，抛一个骰子两次，总共会出现6*6=36种可能的结果（备注：这里涉及到排列组合的相关知识），我们可以用之前表示的那样用一个有序实数对表示，（1，2），（1，1），当然也可以用1，2，3，4，5，6，……，36这36个数字来表示，原则是只要不引起歧义就可以。如果我们用有序实数对表示，那么就可以认为映射到了xy平面上，如果用36个数字表示就可以认为映射到了实数轴上。如果抛n次骰子，那么我们可以用一个有序的n维向量来表示，从而映射到了n维空间上。进一步的，我们有理由认为数轴其实是`$\mathbb{R}^1$`空间，xy轴是`$\mathbb{R}^2$`空间，直到`$\mathbb{R}^n$`空间。这也从侧面说明了概率论和代数以及几何上的某种联系性。

示例3，老张上班时间，我们可以映射到数轴7-9上，即

`$$\Omega = \{x|7\leq x\leq 9, x\in\mathbb{R}\}$$`

如果映射成下面的形式，也是可以的，只不过前面第一种方式最容易被大家所接受。

`$$\Omega = \{x|70\leq x\leq 90, x\in\mathbb{R}\}$$`

`$$\Omega = \{x|6\leq x\leq 888, x\in\mathbb{R}\}$$`

李四的样本空间则可以表示为

$$\Omega = \{x|0\leq x < 24, x\in\mathbb{R}\}$$

需要指出的是，时间是60进制，而数轴是10进制，因此需要涉及转换，比如`$T=7:52$`，按照我们第一种随机变量的表示方式为`$x=7+52/60$`。

通过随机变量和随机事件的映射，随机事件的概率就可以通过随机变量进行转化后计算，比如张三上班的随机事件A，就是

$$P(A) = P\{T=8:00\} = P\{x=8\}$$

从而实现了在数学上把各种复杂的随机事件转化成比较标准和统一的数学表示形式：随机变量来进行运算。

在历史上概率一词有很多种定义方法，其中包括频率定义法、公理化定义法等。其中频率定义法比较直观，但不严谨，公理化定义严谨但不直观。不过他们之间可以互为补充，相得益彰。

首先给出概率的频率定义（统计定义）：

在相同条件下进行了`$n$`次实验，若事件A在`$n$`次实验中发生的次数为`$n_A$`，其比值`$n_A/n$`记为事件A发生的频数，记作`$f_n(A)$`，随着`$n$`的增大，频数`$f_n(A)$`在某一常数附近摆动，记此常数为事件A的概率。

此时我们可以联系微积分的相关内容来进行对比关联和理解，比如一个密度不均匀的细长杆，其质量的计算方法，当我们分割的足够密时，也就是分的个数非常多时（对比上面就是`$n$`的增大），其小块内的密度可以认为是不变的，进行求和后会趋于一个常数，我们认为这个常数就是杆的质量。类似还有圆的面积求法等等。

不均匀密度细杆长

`$$\lim_{n\rightarrow +\infty}\frac{f(n)}{n}= m$$`

圆的面积

`$$\lim_{n\rightarrow +\infty} f(*)= \pi r^2$$`

微积分的思想在于用一个极限的形式来表示一个确定的数，比如圆的面积和杆的质量都是一个确定的数。最典型的极限表示数的例子，如

`$$\lim_{x\rightarrow +\infty}\left(1+\frac{1}{x} \right)^x = e$$`

我们既可以用`$\pi r^2$`来表示圆的面积，也可以用`$\lim_{n\rightarrow +\infty}$`来表示，但是有些确定的数是没有公式的，比如细杆的质量，只能用极限的思路来进行计算。在数学上用严谨的极限语言来描述

`$$\lim_{n\rightarrow +\infty}f(n)= a$$`

就是：`$\forall \varepsilon >0, \exist N\in\Z,$`当`$n>N$`时，恒有`$|f(n)-a|<\varepsilon$`成立。

由此联系起来，概率值也是一个确定的0到1之间的数。随着`$n\rightarrow +\infty$`趋向稳定的一个数。类比极限收敛的一般表述方法，在概率论中频数收敛到稳定的常数值，也可以用类似分析学的语言描述，那就是依概率收敛。首先给出依概率收敛的含义：

设`$Y_1,Y_2,\ldots,Y_n$`是一个随机变量序列，`$a$`是一个常数，若对任意正数`$\varepsilon$`有

`$$\lim_{n\rightarrow +\infty}P\{|Y_n-a|<\varepsilon\}=1$$`

则称序列依概率收敛到`$a$`。

由于随机实验的重复独立性，根据大数定律我们就可以表述出概率的频率定义的极限表述形式，即

`$$\lim_{n\rightarrow +\infty}P\{\left|\frac{n_A}{n}-a\right|<\varepsilon\} = 1$$`

则`$a$`就为事件`$A$`发生的概率。不难发现，我们从微积分的角度出发给出了类似极限的表述方式，也就是概率的频率定义在数学上的合理性，其实这就是概率论中的大数定律。

接下来再解释概率的公理化定义：

设任意随机实验`$E$`，`$\Omega$`为相应的样本空间，若对任意事件`$A$`有唯一实数`$P(A)$`与之对应，且满足下面条件，则这个实数称为A发生的概率。

1. 非负性公理。对任意`$A$`，总有`$P(A)\geq 0$`；
2. 规范性公理。`$P(\Omega)=1$`;
3. 可列可加性公理。

从公理化定义来看，概率这一含义已经不再直观。我们只能知道概率值是一个实数，然而这却是符合现代数学的表述方法。如果我们把`$\Omega$`看成全集，事件$A$看成子集，再加上集合到实数的映射，也就是测度，三位一体`$(\Omega, \mathscr{F}, P)$`称为概率测度空间。`$\mathscr{F}$`中的集合`$A$`又称为随机事件。建立在测度论基础上的概率论从而有了更加严谨的数学基础。

上面已经交代过在一维情况下，随机事件可以映射到数轴上，比如张三上班时间可以认为是集合`$\Omega=[7,9]=\{x|7\leq x\leq 9,x\in\R\}$`，这里暂时先不考虑集合的开闭。类似的在微积分中，我们也可以把杆映射到数轴上，从而得到杆上每个位置上的密度分布情况。张三8点前上班的概率就对应子集`$A=[7,8]$`，其概率对应一个数，记为`$P(A)=P\{7\leq X \leq 9\}$`，这里`$X$`表示随机变量。同样的，杆的“一部分”也有质量，对应的实数称为质量。在概率论中`$P\{\Omega\}=1$`，就相当于我们把杆的整个质量归一化到1。我们知道，长度，面积，体积，质量这些具有比较明确的物理和现实意思的在数学上都称为“测度”，现在人们也把概率当成一种“测度”和这些一并统一了起来。从而概率论就和其它学科一样，可以讨论诸如收敛性、可导可微，连续性等等。这或许就是数学的自恰以及能统御万事万物的魅力所在吧。

